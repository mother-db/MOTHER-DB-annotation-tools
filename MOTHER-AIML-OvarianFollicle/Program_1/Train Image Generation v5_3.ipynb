{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a95f1c5",
   "metadata": {
    "id": "87d663e8"
   },
   "source": [
    "# MOTHER-DB project\n",
    "\n",
    "Image segmentation of ovarian follicles, part of https://mother-db.org/\n",
    "\n",
    "**Program 1, Train Image Generation.ipynb, Version 5_3,  March 7, 2025:**<BR>\n",
    "Creation of sub-images from a set of annotated histology images and files of annotations.\n",
    "\n",
    "## About this project\n",
    "This project developed uses AI/ML techniques to segment histology images from the ovaries of nonhuman primates. Specifically, this suite of programs attempts to identify the following six follicle types: \n",
    " 1. Primordial\n",
    " 1. Transitional Primordial\n",
    " 1. Primary\n",
    " 1. Transitional Primary\n",
    " 1. Secondary\n",
    " 1. Multilayer\n",
    " \n",
    "The follicle type definitions are based on the recommendations of the NICHD-Sponsored Ovarian Nomenclature \n",
    "Workshop committee for primates.\n",
    " 1. Yano Maher JC, Zelinski MB, Oktay KH, Duncan FE, Segars JH, Lujan ME, Lou H, Yun B, Hanfling SN, Schwartz LE, \n",
    "Laronda MM, Halvorson LM, O'Neill KE, Gomez-Lobo V. Fertil Steril. 2024 Nov 14:S0015-0282(24)02394-X. \n",
    "doi: 10.1016/j.fertnstert.2024.11.016. Epub ahead of print. PMID: 39549739. https://pubmed.ncbi.nlm.nih.gov/39549739/  \n",
    " \n",
    "## About this program module\n",
    "\n",
    "This program takes as input a set of ovarian histology slides along with annotation files and creates sub-images of \n",
    "each annotation. In addition, the program creates \"augmentations\" of the images by rotating, flipping and offsetting \n",
    "the images. The output is a set of folders, one for each follicle type, containing the sub-images. \n",
    "\n",
    "In addition, this program creates a set of \"negative\" sub-images, which are random samples of the original image but \n",
    "not too near an existing annotation. The minimal allowed distance between a randomly selected negative image \n",
    "and an existing image is set to be 1/4 of the sub-image width (50 pixels for sub-images that are 200 pixels wide). \n",
    "The number of negatives generated is based on the number of actual annotations, as defined by the parameter \n",
    "`negatives_per_annot`. A \"blank\" image is defined as any sub-image that has a standard deviation of the pixels in \n",
    "the sub-image that is less than a threshold. Depending on the amount of non-tissue area in the original slide, this \n",
    "process can generate an excessive number of blank images. To counter this, the number of blank negatives is reduced. \n",
    "\n",
    "The output sub-images filenames include information necessity to trace a particular sub-image back to the \n",
    "original annotation and histology slide. \n",
    "\n",
    "## About the data\n",
    "\n",
    "The data consists of paired images and annotation files. For example, the image file `14736_UN_050a.ome.tif` and \n",
    "its paired annotations file `14736_UN_050a.annotations.txt`. Note that all the image/annotation file pairs are \n",
    "in the same directory.\n",
    "\n",
    "The path to the paired images, along with other information such as image resolution, is described in the `parameters.py` file.\n",
    " \n",
    "\n",
    "## About the MOTHER project and MOTHER-DB\n",
    "\n",
    "The Multispecies Ovary Tissue Histology Electronic Repository (MOTHER) provides public access to digitized microscopic images of ovary tissues along with information that ensures image integrity and quality. Currently, there is no electronic repository of ovary histology slides that preserves these valuable research collections for future generations. MOTHER is a web-accessible, open resource for scientists, educators, and the public to stimulate collaboration and scientific research. Educators may use the slide images in a range of courses from reproductive biology to teaching computerized image analysis.\n",
    "\n",
    "Biology is increasingly dependent upon quantitative data analysis, and MOTHER should inspire computational thinking in biology broadly, while developing specific skills in microscopy, computer programming, and data and image analysis.\n",
    "\n",
    "## License For Use\n",
    "\n",
    "This work is licensed under CC BY-NC-SA 4.0. To view a copy of this license, \n",
    "visit https://creativecommons.org/licenses/by-nc-sa/4.0/\n",
    "\n",
    "\n",
    "## Funding\n",
    "\n",
    "MOTHER-DB, and this project was funded by \n",
    " * Grant “CIBR Multispecies Ovary Tissue Histology Electronic Repository (MOTHER)” from the National Science Foundation (NSF DBI-2054061, 2021 – 2024). \n",
    " * Indiana University, Faculty Assistance in Data Science (FADS) Project\n",
    " * Arizona State University\n",
    "\n",
    "## Contributors\n",
    "Many people have contributed to this project:\n",
    "\n",
    " * Code development\n",
    "   * James Sluka, Indiana University\n",
    "   * Karen Watanabe, Arizona State University\n",
    "   * Riley Israels, Arizona State University\n",
    "   * Parth Ravindra Rao, Indiana University \n",
    "   * Param Nagda, Indiana University\n",
    "   * Colette Lund, Arizona State University\n",
    "\n",
    " * Training data creation\n",
    "   * Mary Zelinski, Oregon National Primate Research Center\n",
    "   * Karen Watanabe, Arizona State University\n",
    "   * Numerous Arizona State undergraduate and graduate students\n",
    " \n",
    "## Program notes\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Special Note:</b> <br>\n",
    "The annotations from QuPath and the MOTHER annotators in the text files give annotation centers in micrometers \n",
    "from the origin and **not** in pixels from the origin.\n",
    "</div>\n",
    "\n",
    "### An input file 'filename' produces the following set of augmentation image files (16 files per annotation)\n",
    "For one of the annotations in the original image file `21930_LT_060a.ome`, of `Primordial` type, with center \n",
    "(in microns, not pixels) of `x2643_y8070` and sub-image window 200 pixels wide:\n",
    "\n",
    "<pre style=\"padding: 0px; margin: 0px;\" >\n",
    "\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200orig_ang0.png           # original image, centroid is 2643,8070 um\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200orig_ang180.png         # rotated 180 degrees (CCW)\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200orig_ang270.png         # rotated 270 degrees (CCW)\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200orig_ang90.png          # rotated  90 degrees (CCW)\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200orig_horiz_ang0.png     # original image, flipped horizontally\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200orig_horiz_ang180.png   # ... then rotated 180 degrees (CCW)\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200orig_horiz_ang270.png\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200orig_horiz_ang90.png\n",
    "\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200offs-07-05_ang0.png    # image offset by -7,-5 pixels (randomly selected)\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200offs-07-05_ang180.png  # ... then rotated 180 (CCW)\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200offs-07-05_ang270.png\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200offs-07-05_ang90.png\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200offs-07-05_horiz_ang0.png    # image offset, then flipped horizontally\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200offs-07-05_horiz_ang180.png  # ... then rotated 180 (CCW)\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200offs-07-05_horiz_ang270.png\n",
    "21930_LT_060a.ome_Primordial_x2643_y8070_w200offs-07-05_horiz_ang90.png\n",
    "\n",
    "</pre>\n",
    "#### In addition, \"Negative\" files are created with similar naming conventions.\n",
    "    \n",
    "The file name contains \"Negative\" in place of a follicle type. The number of negatives generated is based on the \n",
    "number of actual annotations. This is set by the parameter `negatives_per_annot`, which is typically set to 4. Each\n",
    "Negative image is also augmented in the same way the regular annotations are.\n",
    "\n",
    "#### Other options for generating augmentations:\n",
    "\n",
    "##### make_flipped_rgb\n",
    "\n",
    "The option `make_flipped_rgb`, if set to be `True`, will generate an additional set of 16 images per annotation where the \n",
    "red and blue color channels in the sub-image are swapped. The resulting filenmes will have __flippedRGB__ as the last\n",
    "part of their filename.\n",
    "\n",
    "0381_RT_200c.ome_Multilayer_x9508_y6001_w200orig_ang0__flippedRGB__.png\n",
    "\n",
    "\n",
    "##### make_grayscale\n",
    "\n",
    "The option `make_grayscale`, if set to be `True`, will generate an aditonal set of 16 images per annotation where \n",
    "sub-image has been converted to gray scale. The resulting filenmes will have __grayscale__ as the last\n",
    "part of their filename.\n",
    "    \n",
    "30381_RT_200c.ome_Multilayer_x9508_y6001_w200orig_ang0_grayscale.png\n",
    "    \n",
    "\n",
    "If both `make_flipped_rgb` and `make_grayscale` options are selected then a total of 48 images are created for \n",
    "each annotation. \n",
    "\n",
    "The `parameters.py` file contains the paths to folders containing the histology images and annotations. In addition, this file contains the image resolution for each file.\n",
    "\n",
    "The `paramters.py` file includes an array, **types**, consisting of the names of different follicle types. Also, another \n",
    "list is created named **file_list** where multiple dictionaries are created. Each dictionary entry consists of the path of \n",
    "where the image and annotation file is and the conversion ratio to convert the x and y coordinates to pixel values.\n",
    "\n",
    "### Outputs\n",
    "This program creates a time stamped folder (e.g., `Train Images_2025-03-08_23-41-39`), The \n",
    "`Train Images_YYYY-MM-DD_hh-mm-ss` folder will contain subfolders for each of the follicle types being processed. \n",
    "Within those folder will be sub-images. This folder is located in the same folder as this code.\n",
    "\n",
    "In addtion, a text file ('work_log.txt') summarizing the process, a file that contains the path to the projectd notes\n",
    "folder (`work_log_filename.txt`), and an HTML version of the final state of this jupyter notebook are also created.\n",
    "\n",
    "### Changes:\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Special Note:</b> <br>\n",
    "The sub-window size is defined for each follicle type in the parameters.py file. This should work well for \n",
    "single class classifiers (\"hotdog-not-a-hotdog\"). But for <b>multi-class classifiers it is likely that all \n",
    "the sub-window sizes should be the same.</b> This can be done in a project-specific parameters.py file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5434e0",
   "metadata": {
    "id": "3c5434e0"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43278e68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "executionInfo": {
     "elapsed": 1237,
     "status": "error",
     "timestamp": 1686071808145,
     "user": {
      "displayName": "James Sluka",
      "userId": "11089343449605785062"
     },
     "user_tz": 240
    },
    "id": "43278e68",
    "outputId": "24fa30aa-235b-4a69-9a0d-22f7e2038d11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsluka\\OneDrive - Indiana University\\Desktop\\Work\\Watanabe ovary 2021\\MOTHER\\AIML_code_for_Paper\\Program 1\n",
      "file_base:\n",
      " ./TrainingData_20250409/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2  ###  opencv-python image handling\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import imutils  ### additional image handling tools\n",
    "import shutil\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "# Install conda package for ipylab in the current Jupyter kernel\n",
    "### !conda install --yes --prefix {sys.prefix} -c conda-forge ipylab\n",
    "from ipylab import JupyterFrontEnd\n",
    "\n",
    "import parameters  ### parameters.py contains info for this particular MOTHER ML segmentation project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c0a3fd1-56c1-4151-935c-80c3c517b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_flipped_rgb = False  # False\n",
    "make_grayscale = False  # False\n",
    "negatives_per_annot = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27eb6e",
   "metadata": {},
   "source": [
    "### Echo some info from the parameters.py file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e530d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'angles', 'create_folder', 'datetime', 'file_base', 'file_list', 'os', 'prediction_follicle', 'types']\n",
      "\n",
      "parameters.angles:\n",
      " [90, 180, 270]\n",
      "\n",
      "parameters.file_list:\n",
      " # Image path                                         Annotation path                                    Resolution\n",
      " 1 ./TrainingData_20250409/14736_UN_050a.ome.tif      ./TrainingData_20250409/14736_UN_050a.annotations.txt   0.690\n",
      " 2 ./TrainingData_20250409/16418_UN_140b.ome.tif      ./TrainingData_20250409/16418_UN_140b.annotations.txt   0.690\n",
      " 3 ./TrainingData_20250409/19006_UN_020a.ome.tif      ./TrainingData_20250409/19006_UN_020a.annotations.txt   0.690\n",
      " 4 ./TrainingData_20250409/25058_LT_005a.ome.tif      ./TrainingData_20250409/25058_LT_005a.annotations.txt   0.690\n",
      " 5 ./TrainingData_20250409/25065_LT_010a.ome.tif      ./TrainingData_20250409/25065_LT_010a.annotations.txt   0.690\n",
      " 6 ./TrainingData_20250409/25081_LT_010a.ome.tif      ./TrainingData_20250409/25081_LT_010a.annotations.txt   0.690\n",
      " 7 ./TrainingData_20250409/27570_UN_110a.ome.tif      ./TrainingData_20250409/27570_UN_110a.annotations.txt   0.690\n",
      " 8 ./TrainingData_20250409/30381_RT_070b.ome.tif      ./TrainingData_20250409/30381_RT_070b.ome.annotationsTable.txt   0.690\n",
      " 9 ./TrainingData_20250409/30381_RT_140b.ome.tif      ./TrainingData_20250409/30381_RT_140b.ome.annotationsTable.txt   0.690\n",
      "10 ./TrainingData_20250409/30381_RT_200c.ome.tif      ./TrainingData_20250409/30381_RT_200c.ome.annotationsTable.txt   0.690\n",
      "11 ./TrainingData_20250409/32002_RT_050a.ome.tif      ./TrainingData_20250409/32002_RT_050a.ome.annotationsTable.txt   0.690\n",
      "12 ./TrainingData_20250409/32002_RT_110b.ome.tif      ./TrainingData_20250409/32002_RT_110b.ome.annotationsTable.txt   0.690\n",
      "13 ./TrainingData_20250409/32002_RT_160c.ome.tif      ./TrainingData_20250409/32002_RT_160c.ome.annotationsTable.txt   0.690\n",
      "14 ./TrainingData_20250409/33564_RT_060a.ome.tif      ./TrainingData_20250409/33564_RT_060a.ome.annotationsTable.txt   0.690\n",
      "15 ./TrainingData_20250409/33564_RT_120b.ome.tif      ./TrainingData_20250409/33564_RT_120b.ome.annotationsTable.txt   0.690\n",
      "16 ./TrainingData_20250409/33564_RT_180b.ome.tif      ./TrainingData_20250409/33564_RT_180b.ome.annotationsTable.txt   0.690\n",
      "17 ./TrainingData_20250409/21930_LT_060a.ome.tif      ./TrainingData_20250409/21930_LT_060a_annotationsTable.txt   0.690\n",
      "18 ./TrainingData_20250409/21930_LT_120b.ome.tif      ./TrainingData_20250409/21930_LT_120b_annotationsTable.txt   0.690\n",
      "\n",
      "parameters.types:\n",
      "               Primordial  {'coordinates': [], 'window_size': 200, 'offset': 20.0}\n",
      "  Transitional Primordial  {'coordinates': [], 'window_size': 200, 'offset': 20.0}\n",
      "                  Primary  {'coordinates': [], 'window_size': 200, 'offset': 20.0}\n",
      "     Transitional Primary  {'coordinates': [], 'window_size': 200, 'offset': 20.0}\n",
      "                Secondary  {'coordinates': [], 'window_size': 200, 'offset': 20.0}\n",
      "               Multilayer  {'coordinates': [], 'window_size': 200, 'offset': 20.0}\n"
     ]
    }
   ],
   "source": [
    "print(dir(parameters))\n",
    "print('\\nparameters.angles:\\n',parameters.angles)\n",
    "print('\\nparameters.file_list:')  \n",
    "print(\"%2s %-50s %-50s %s\" % (\"#\",\"Image path\",\"Annotation path\",\"Resolution\"))\n",
    "for iii in range(len(parameters.file_list)):\n",
    "    print(\"%2i %-50s %-50s   %5.3f\" \\\n",
    "          % (iii+1,parameters.file_list[iii][\"Image path\"],parameters.file_list[iii][\"Annotation path\"],parameters.file_list[iii][\"Resolution\"],))\n",
    "print('\\nparameters.types:')    \n",
    "for k in parameters.types:\n",
    "    print('%25s  %50s' % (k,parameters.types[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed954507",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "Enter the follicle type that will be processed (for \"hotdog-not-a-hotdog\" single classifiers).<br>\n",
    "\n",
    "__Note 1: This may be overriden by setting in the parmaters.py file. In particular, when doing multiclassifictions the list of follicle types is defined in the parameters.py file.__\n",
    "\n",
    "__Note 2: You can enter any follicle type if you are doing a multiclassifier. But the sub-image `window_size` and `offset` will be based on the follicle type entered here. For the 6 small follicle types enter _\"Primordial\"_ or just hit enter to accept the default.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc15bb2",
   "metadata": {
    "id": "ceb199b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose a follicle type from:\n",
      "    dict_keys(['Primordial', 'Transitional Primordial', 'Primary', 'Transitional Primary', 'Secondary', 'Multilayer']) \n",
      "or \"Multiple\"\n",
      "Enter the type of follicle for the model[default=Multiple]:Primary\n",
      "\n",
      "prediction_follicle type =  Primary\n"
     ]
    }
   ],
   "source": [
    "follicle_type = parameters.prediction_follicle()  # this is a function in the parameters.py file\n",
    "follicle_type2 = follicle_type\n",
    "if follicle_type == \"Multiple\":\n",
    "    follicle_type = \"Primary\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df751283",
   "metadata": {
    "id": "df751283"
   },
   "source": [
    "## Functions\n",
    "Here a function called **offset_images** is created where a subimage of a follicle type is moved by x and y pixel which are randomly selected from a range of 1 to *offset_length* (typically 1 to 25) and n random offsets are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b08320c",
   "metadata": {
    "id": "2b08320c"
   },
   "outputs": [],
   "source": [
    "def offset_images(n, centroids, img, offset_length):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        #x = random.randrange(-1,2,2)*random.randint(1,int(offset_length))\n",
    "        #y = random.randrange(-1,2,2)*random.randint(1,int(offset_length))\n",
    "        x = random.choice((-1,1))*random.randint(1,int(offset_length))  # 5_2: choice is easier to understand than randrange\n",
    "        y = random.choice((-1,1))*random.randint(1,int(offset_length))  # 5_2: choice is easier to understand than randrange\n",
    "        #if 0 < x < len(img[0]) and 0 < y < len(img):\n",
    "        if 0 < centroids[0][0]+y < img.shape[0] and 0 < centroids[0][1]+x < img.shape[1]:  # 5_2\n",
    "            centroids.append((centroids[0][0]+y, centroids[0][1]+x))\n",
    "            i += 1\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09edf693",
   "metadata": {
    "id": "09edf693"
   },
   "source": [
    "Here the **random_image_generation** function is created to produce random images that are not near images of any follicle type. \n",
    "* We have make sure that the center coordinates of the random image are *minOffsetExistingAnnot* pixels away from any of the subimages of a follicle. \n",
    "  * ONLY need to check the distances to images used for this particular classifier. \n",
    "  * Do not need to check distance to say the Corpus Leuteum.\n",
    "* No image can be close than *minOffsetImageEdge* to the edge of the full image\n",
    "* Each time this is called it returns ONE image \n",
    "\n",
    "Input and output\n",
    "* *img* is the full image\n",
    "* *df* is the dataframe containing all the annotation's coordinates\n",
    "* returns the x,y cordinates of an aacceptable random image for use as a negative.\n",
    "\n",
    "___The distance of the center of the random image to any existing image is currently fixed at 50 pixels. This might be too close for the larger follicles like antral. Perhaps the minimum distance should be based on the follicle's sub-window size. For example, Primordial are 100 pixesl wide so a 50 pixel offset it pretty big. But Antrals are 1800 pixesl wide, so 50 pixels is too small. So perhaps instead use a minimum distance that is 50% of the sub-window size for the follicle type?___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8885ac",
   "metadata": {
    "id": "ec8885ac"
   },
   "outputs": [],
   "source": [
    "def random_image_generation(img, df, width):\n",
    "    minOffsetImageEdge = int(width / 2) + int(width * 0.1) # was 100 in 5_1, no sub-image center can be closer than this to the edge of the full image\n",
    "    minOffsetExistingAnnot = int(width / 4) #+ int(width * 0.1) # no sub-image center can be closer than this to the center of an annotation\n",
    "\n",
    "    total_annots = df[['Centroid X pixels', 'Centroid Y pixels']].dropna().to_numpy()\n",
    "    \n",
    "    valid = False\n",
    "    fails = 0\n",
    "    \n",
    "    while not valid:\n",
    "        random_row = np.random.randint(minOffsetImageEdge, img.shape[0] - minOffsetImageEdge)\n",
    "        random_col = np.random.randint(minOffsetImageEdge, img.shape[1] - minOffsetImageEdge)\n",
    "\n",
    "        point_compare = []\n",
    "        \n",
    "        for annot in total_annots:\n",
    "            point_compare.append([not (annot[0] - minOffsetExistingAnnot <= random_col <= annot[0] + minOffsetExistingAnnot), not (annot[1] - minOffsetExistingAnnot <= random_row <= annot[1] + minOffsetExistingAnnot)])\n",
    "\n",
    "        point_compare = np.array(point_compare)\n",
    "\n",
    "        valid = point_compare.any(axis = 1).all()\n",
    "        \n",
    "        (_, _, ratio) = ImageMeanSD(img[random_row - int(width / 2):random_row + int(width / 2), random_col - int(width / 2):random_col + int(width / 2), :])\n",
    "        \n",
    "        #Retry 95% of blank images\n",
    "        if ratio < 0.02 and np.random.random() > 0.05:\n",
    "            valid = False\n",
    "        \n",
    "        fails += 1\n",
    "        if fails > 1000:\n",
    "            #print(\"Failure\")\n",
    "            return 0, 0\n",
    "    \n",
    "    #print(\"Success\")\n",
    "    return random_row, random_col\n",
    "    \n",
    "    #print('random_image_generation: random_row, random_col',random_row, random_col)\n",
    "    #return random_row, random_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b33d40",
   "metadata": {
    "id": "31b33d40"
   },
   "source": [
    "The **data** function is the main function where all the subimages are generated. Here, we read the image and annotation file from file_list, and convert the x and y coordinates to pixel values. We also create a new folder apart from the folders of each follicle called Negative that consists of the random images generated using random_image_generation function. Now, we get the x,y coordinates from the annotation file and obtain a 150x150 (actual sized defined in the paramters.py file) subimage with the x,y coordinate being the center coordinate. Offset images are generated by offsetting the subimage by random pixels as mentioned above. \n",
    "\n",
    "Along with the original subimage and offset images we also generate random images. The random images are stored in the negative images folder and the rest of the images are stored in the folder of the follicle type of the original subimage. For all the images horizontal flips are obtained and each subimage and its horizontal subimage are rotated by 90, 180 and 270 degrees. _The default in the data call is that one random image is generated for each input annotation?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bbc7d5f",
   "metadata": {
    "id": "2bbc7d5f"
   },
   "outputs": [],
   "source": [
    "def data(im_path, ann, conv_ratio, width, n=1):\n",
    "    img = cv2.imread(im_path)\n",
    "    print(\"      image dimensions:\",img.shape)\n",
    "    df = pd.read_csv(ann, sep='\\t')\n",
    "\n",
    "    #Grab only follicles in parameters.types from annotations file\n",
    "    df = df[df['Class'].isin(parameters.types.keys())].reset_index(drop = True)\n",
    "    \n",
    "    df = df.drop(['Image','Parent','Class','Num points'],axis=1)\n",
    "    df['Centroid X pixels'] = df['Centroid X µm'] / conv_ratio\n",
    "    df['Centroid Y pixels'] = df['Centroid Y µm'] / conv_ratio\n",
    "    img_name = im_path.split('.tif')[0]\n",
    "    img_name = os.path.basename(img_name) # needed since we have a path to the orignal images and annotations\n",
    "    print(\"      lines in annotations=\",len(df))\n",
    "    \n",
    "    if 'Negative' not in os.listdir(Work_Folder):\n",
    "        path_random = os.path.join(Work_Folder,'Negative')\n",
    "        os.mkdir(path_random)\n",
    "\n",
    "    subImageFails = 0\n",
    "    subImageFailsList = []\n",
    "\n",
    "    #negative_centers = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i]['Name'] in parameters.types:  # is the annotation for one of the named follicle types?\n",
    "            if df.loc[i]['Name'] not in os.listdir(Work_Folder): # create folder for this follicle type if it does not already exist\n",
    "                path = os.path.join(Work_Folder,df.loc[i]['Name'])\n",
    "                os.mkdir(path)\n",
    "                print(\"         new dir:\",path)\n",
    "\n",
    "            col = int(df.loc[i]['Centroid X pixels'])\n",
    "            row = int(df.loc[i]['Centroid Y pixels'])\n",
    "            centroids = [(row,col)]\n",
    "\n",
    "            path = os.path.join(Work_Folder,df.loc[i]['Name'], img_name)\n",
    "            path_random = os.path.join(Work_Folder,'Negative', img_name)\n",
    "\n",
    "            # update the centroids with the random offsets etc.\n",
    "            centroids = offset_images(1, centroids, img, offset_length = parameters.types[follicle_type]['offset'])\n",
    "\n",
    "            for j in range(len(centroids)):\n",
    "                if    (parameters.types[follicle_type]['window_size']//2 < centroids[j][0] < len(img)    - parameters.types[follicle_type]['window_size']//2) \\\n",
    "                  and (parameters.types[follicle_type]['window_size']//2 < centroids[j][1] < len(img[0]) - parameters.types[follicle_type]['window_size']//2):\n",
    "                    parameters.types[df.loc[i]['Name']]['coordinates'].append(centroids[j])\n",
    "                    if j == 0:\n",
    "                        tag = 'orig'\n",
    "                        subimg = img[centroids[j][0]-int(width/2):centroids[j][0]+int(width/2), centroids[j][1]-int(width/2):centroids[j][1]+int(width/2), :]\n",
    "                        img_h = cv2.flip(subimg,1)\n",
    "                        fpath = path + '_' + df.loc[i]['Name'] + '_x' + str(col) + '_y' + str(row) + '_w' + str(width) + tag\n",
    "                        cv2.imwrite(fpath + '_ang0.png',subimg)\n",
    "                        cv2.imwrite(fpath + '_horiz_ang0.png',img_h)\n",
    "\n",
    "                        if make_flipped_rgb:\n",
    "                            #Flip RGB values and save -RI\n",
    "                            cv2.imwrite(fpath + '_ang0_flippedRGB.png', cv2.cvtColor(subimg, cv2.COLOR_BGR2RGB))\n",
    "                            cv2.imwrite(fpath + '_horiz_ang0_flippedRGB.png', cv2.cvtColor(img_h, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                        if make_grayscale:\n",
    "                            #Grayscale images and save -RI\n",
    "                            cv2.imwrite(fpath + '_ang0_grayscale.png', cv2.cvtColor(cv2.cvtColor(subimg, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                            cv2.imwrite(fpath + '_horiz_ang0_grayscale.png', cv2.cvtColor(cv2.cvtColor(img_h, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                            \n",
    "                        for k in parameters.angles:\n",
    "                            cv2.imwrite(fpath + '_ang' + str(k) + '.png',imutils.rotate(subimg,angle=k))\n",
    "                            cv2.imwrite(fpath + '_horiz_ang' + str(k) + '.png',imutils.rotate(img_h,angle=k))\n",
    "\n",
    "                            if make_flipped_rgb:\n",
    "                                #Flip RGB values and save -RI\n",
    "                                cv2.imwrite(fpath + '_ang' + str(k) + '_flippedRGB.png', cv2.cvtColor(imutils.rotate(subimg,angle=k), cv2.COLOR_BGR2RGB))\n",
    "                                cv2.imwrite(fpath + '_horiz_ang' + str(k) + '_flippedRGB.png', cv2.cvtColor(imutils.rotate(img_h,angle=k), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                            if make_grayscale:\n",
    "                                #Grayscale images and save -RI\n",
    "                                cv2.imwrite(fpath + '_ang' + str(k) + '_grayscale.png', cv2.cvtColor(cv2.cvtColor(imutils.rotate(subimg,angle=k), cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                                cv2.imwrite(fpath + '_horiz_ang' + str(k) + '_grayscale.png', cv2.cvtColor(cv2.cvtColor(imutils.rotate(img_h,angle=k), cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                    \n",
    "                    else:\n",
    "                        tag = 'offs'\n",
    "                        #print('\\t\\tcent off centroids[j]:',centroids[j])\n",
    "                        offset_valsStr = str(centroids[0][1]-centroids[j][1]).zfill(3) + str(centroids[0][0]-centroids[j][0]).zfill(3)  # 5_2 the x and y offsets paded with leading zero\n",
    "                        #print('\\t\\toffset_valsStr:',offset_valsStr)\n",
    "                        subimg = img[centroids[j][0]-int(width/2):centroids[j][0]+int(width/2), centroids[j][1]-int(width/2):centroids[j][1]+int(width/2), :]\n",
    "                        img_h = cv2.flip(subimg,1)\n",
    "                        fpath = path + '_' + df.loc[i]['Name'] + '_x' + str(col) + '_y' + str(row) + '_w' + str(width)+ tag\n",
    "                        cv2.imwrite(fpath + offset_valsStr + '_ang0.png',subimg)\n",
    "                        cv2.imwrite(fpath + offset_valsStr + '_horiz_ang0.png',img_h)\n",
    "\n",
    "                        if make_flipped_rgb:\n",
    "                            #Flip RGB values and save -RI\n",
    "                            cv2.imwrite(fpath + offset_valsStr + '_ang0_flippedRGB.png', cv2.cvtColor(subimg, cv2.COLOR_BGR2RGB))\n",
    "                            cv2.imwrite(fpath + offset_valsStr + '_horiz_ang0_flippedRGB.png', cv2.cvtColor(img_h, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                        if make_grayscale:\n",
    "                            #Grayscale images and save -RI\n",
    "                            cv2.imwrite(fpath + offset_valsStr + '_ang0_grayscale.png', cv2.cvtColor(cv2.cvtColor(subimg, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                            cv2.imwrite(fpath + offset_valsStr + '_horiz_ang0_grayscale.png', cv2.cvtColor(cv2.cvtColor(img_h, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                            \n",
    "                        for k in parameters.angles:\n",
    "                            cv2.imwrite(fpath + offset_valsStr + '_ang' + str(k) + '.png',imutils.rotate(subimg,angle=k))\n",
    "                            cv2.imwrite(fpath + offset_valsStr + '_horiz_ang' + str(k) + '.png',imutils.rotate(img_h,angle=k))\n",
    "\n",
    "                            if make_flipped_rgb:\n",
    "                                #Flip RGB values and save -RI\n",
    "                                cv2.imwrite(fpath + offset_valsStr + '_ang' + str(k) + '_flippedRGB.png', cv2.cvtColor(imutils.rotate(subimg,angle=k), cv2.COLOR_BGR2RGB))\n",
    "                                cv2.imwrite(fpath + offset_valsStr + '_horiz_ang' + str(k) + '_flippedRGB.png', cv2.cvtColor(imutils.rotate(img_h,angle=k), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                            if make_grayscale:\n",
    "                                #Grayscale images and save -RI\n",
    "                                cv2.imwrite(fpath + offset_valsStr + '_ang' + str(k) + '_grayscale.png', cv2.cvtColor(cv2.cvtColor(imutils.rotate(subimg,angle=k), cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                                cv2.imwrite(fpath + offset_valsStr + '_horiz_ang' + str(k) + '_grayscale.png', cv2.cvtColor(cv2.cvtColor(imutils.rotate(img_h,angle=k), cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                    \n",
    "                else: # annotation is too close to edge of fullimage to make a subwindow    \n",
    "                    print('\\tAnnotation to close to image edge:   ',centroids[j])\n",
    "                    subImageFails += 1\n",
    "                    subImageFailsList.append(centroids[j])\n",
    "                    \n",
    "                    \n",
    "            # generate random images ('Negatives')\n",
    "            for randomCount in range(negatives_per_annot):  # generate this many randoms for each real annotation\n",
    "                # random_row, random_col = random_image_generation(img, df, n) # version 5_0\n",
    "                random_row, random_col = random_image_generation(img, df, width) # version 5_1        \n",
    "                subimg = img[random_row-int(width/2):random_row+int(width/2), random_col-int(width/2):random_col+int(width/2), :]\n",
    "\n",
    "                #negative_centers.append([random_row, random_col])\n",
    "                \n",
    "                # is this an image of just the slide background?\n",
    "                #(mean,sd,ratio) = ImageMeanSD(subimg) #Attempting to handle in random_image_generation()\n",
    "                # accept all images with ratio of sd/mean > 0.1, but only 10% with sd/mean <= 0.1 (omitting some blank images)\n",
    "                #if ratio > 0.01 or np.random.random() <= 0.10:\n",
    "                if True: #Placeholder to prevent major formatting changes\n",
    "                    tag = 'orig'  # 5_2\n",
    "                    img_h = cv2.flip(subimg,1)\n",
    "                    # version 5_1: changed 'row' and 'col' inthe below to 'random_row' and 'random_col'.  JPS\n",
    "                    cv2.imwrite(path_random + '_' + 'Negative' + '_x' + str(random_col) + '_y' + str(random_row) + '_w' + str(width)+ tag + '_ang0.png',subimg)\n",
    "                    cv2.imwrite(path_random + '_' + 'Negative' + '_x' + str(random_col) + '_y' + str(random_row) + '_w' + str(width)+ tag + '_horiz_ang0.png',img_h)\n",
    "                    \n",
    "                    if make_flipped_rgb:\n",
    "                        #Flip RGB values and save -RI\n",
    "                        cv2.imwrite(path_random + '_' + 'Negative' + '_x' + str(random_col) + '_y' + str(random_row) + '_w' + str(width)+ tag + '_ang0_flippedRGB.png', cv2.cvtColor(subimg, cv2.COLOR_BGR2RGB))\n",
    "                        cv2.imwrite(path_random + '_' + 'Negative' + '_x' + str(random_col) + '_y' + str(random_row) + '_w' + str(width)+ tag + '_horiz_ang0_flippedRGB.png', cv2.cvtColor(img_h, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                    if make_grayscale:\n",
    "                        #Grayscale images and save -RI\n",
    "                        cv2.imwrite(path_random + '_' + 'Negative' + '_x' + str(random_col) + '_y' + str(random_row) + '_w' + str(width)+ tag + '_ang0_grayscale.png', cv2.cvtColor(cv2.cvtColor(subimg, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                        cv2.imwrite(path_random + '_' + 'Negative' + '_x' + str(random_col) + '_y' + str(random_row) + '_w' + str(width)+ tag + '_horiz_ang0_grayscale.png', cv2.cvtColor(cv2.cvtColor(img_h, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                        \n",
    "                    for j in parameters.angles:\n",
    "                        cv2.imwrite(path_random + '_' + 'Negative' + '_x' + str(random_col) + '_y' + str(random_row) + '_w' + str(width) + tag + '_ang' + str(j) + '.png',imutils.rotate(subimg,angle=j))\n",
    "                        cv2.imwrite(path_random + '_' + 'Negative' + '_x' + str(random_col) + '_y' + str(random_row) + '_w' + str(width) + tag + '_horiz_ang' + str(j) + '.png',imutils.rotate(img_h,angle=j))\n",
    "\n",
    "                        if make_flipped_rgb:\n",
    "                            #Flip RGB values and save -RI\n",
    "                            cv2.imwrite(path_random + '_' + 'Negative' + '_x' + str(random_col) + '_y' + str(random_row) + '_w' + str(width) + tag + '_ang' + str(j) + '_flippedRGB.png', cv2.cvtColor(imutils.rotate(subimg,angle=j), cv2.COLOR_BGR2RGB))\n",
    "                            cv2.imwrite(path_random + '_' + 'Negative' + '_x' + str(random_col) + '_y' + str(random_row) + '_w' + str(width) + tag + '_horiz_ang' + str(j) + '_flippedRGB.png', cv2.cvtColor(imutils.rotate(img_h,angle=j), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                        if make_grayscale:\n",
    "                            #Grayscale images and save -RI\n",
    "                            cv2.imwrite(path_random + '_' + 'Negative' + '_x' + str(random_col) + '_y' + str(random_row) + '_w' + str(width) + tag + '_ang' + str(j) + '_grayscale.png', cv2.cvtColor(cv2.cvtColor(imutils.rotate(subimg,angle=j), cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                            cv2.imwrite(path_random + '_' + 'Negative' + '_x' + str(random_col) + '_y' + str(random_row) + '_w' + str(width) + tag + '_horiz_ang' + str(j) + '_grayscale.png', cv2.cvtColor(cv2.cvtColor(imutils.rotate(img_h,angle=j), cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                            \n",
    "                    \n",
    "                    # 5_2 added in the missing random offsets for the negative images\n",
    "                    tag = 'offs'  # 5_2\n",
    "                    # below is done here for the random offsets for the negatives, this should\n",
    "                    # match the code that does random offsets for the regular annotations.  ooooooooooooooooooooooooooo\n",
    "                    offset_length = parameters.types[follicle_type]['offset']\n",
    "                    x = random.choice((-1,1))*random.randint(1,int(offset_length))  # 5_2: choice is easier to understand than randrange\n",
    "                    y = random.choice((-1,1))*random.randint(1,int(offset_length))  # 5_2: choice is easier to understand than randrange\n",
    "                \n",
    "                    # random_row, random_col are the centroids for the original negative image\n",
    "                    offset_valsStr = str(x).zfill(3) + str(y).zfill(3)  # 5_2 the x and y offsets paded with leading zero\n",
    "                    subimg = img[random_row-int(width/2)-y:random_row+int(width/2)-y, random_col-int(width/2)-x:random_col+int(width/2)-x, :]\n",
    "                    img_h = cv2.flip(subimg,1)\n",
    "                    #####################\n",
    "                    #print('\\t\\t path_random:',path_random)\n",
    "                    #print(\"df.loc[i]['Name'],random_row,random_col,width,offset_valsStr,img.shape:\",df.loc[i]['Name'],random_row,random_col,width,offset_valsStr,img.shape)\n",
    "                    #####################\n",
    "                    fpath = path_random + '_' + 'Negative' + '_x' + str(random_col) + '_y' + str(random_row) + '_w' + str(width) + tag\n",
    "                    cv2.imwrite(fpath+ offset_valsStr + '_ang0.png',subimg)\n",
    "                    cv2.imwrite(fpath+ offset_valsStr + '_horiz_ang0.png',img_h)\n",
    "                    \n",
    "                    if make_flipped_rgb:\n",
    "                        #Flip RGB values and save -RI\n",
    "                        cv2.imwrite(fpath+ offset_valsStr + '_ang0_flippedRGB.png', cv2.cvtColor(subimg, cv2.COLOR_BGR2RGB))\n",
    "                        cv2.imwrite(fpath+ offset_valsStr + '_horiz_ang0_flippedRGB.png', cv2.cvtColor(img_h, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                    if make_grayscale:\n",
    "                        #Grayscale images and save -RI\n",
    "                        cv2.imwrite(fpath+ offset_valsStr + '_ang0_grayscale.png', cv2.cvtColor(cv2.cvtColor(subimg, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                        cv2.imwrite(fpath+ offset_valsStr + '_horiz_ang0_grayscale.png', cv2.cvtColor(cv2.cvtColor(img_h, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                        \n",
    "                    for j in parameters.angles:\n",
    "                        cv2.imwrite(fpath + offset_valsStr + '_ang' + str(j) + '.png',imutils.rotate(subimg,angle=j))\n",
    "                        cv2.imwrite(fpath + offset_valsStr + '_horiz_ang' + str(j) + '.png',imutils.rotate(img_h,angle=j))\n",
    "\n",
    "                        if make_flipped_rgb:\n",
    "                            #Flip RGB values and save -RI\n",
    "                            cv2.imwrite(fpath + offset_valsStr + '_ang' + str(j) + '_flippedRGB.png', cv2.cvtColor(imutils.rotate(subimg,angle=j), cv2.COLOR_BGR2RGB))\n",
    "                            cv2.imwrite(fpath + offset_valsStr + '_horiz_ang' + str(j) + '_flippedRGB.png', cv2.cvtColor(imutils.rotate(img_h,angle=j), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                        if make_grayscale:\n",
    "                            #Grayscale images and save -RI\n",
    "                            cv2.imwrite(fpath + offset_valsStr + '_ang' + str(j) + '_grayscale.png', cv2.cvtColor(cv2.cvtColor(imutils.rotate(subimg,angle=j), cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                            cv2.imwrite(fpath + offset_valsStr + '_horiz_ang' + str(j) + '_grayscale.png', cv2.cvtColor(cv2.cvtColor(imutils.rotate(img_h,angle=j), cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR))\n",
    "                    \n",
    "    print('\\tTotal subImageFails:',subImageFails,'\\n\\t',subImageFailsList,'\\n')\n",
    "    \n",
    "    #negative_centers = np.array(negative_centers)\n",
    "    #np.save(img_name + \"_negatives.npy\", negative_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b08e1",
   "metadata": {},
   "source": [
    "### Function to Calculate the mean and standard deviation on an image.\n",
    "\n",
    "This is used to identify sub-immages that are pure slide background.<br>\n",
    "The nxnx3 image array is collapsed into a (n*n*s) vector, and we calculate the mean and standard deviation of that vector.<br>\n",
    "Input: a _subimage_ from the __data__ routine.<br>\n",
    "Output: Mean, standard deviation and the ratio mean/(standard deviation)<br>\n",
    "A reasonable cutoff for an all background image is a ratio <= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "097136ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageMeanSD(subimage):\n",
    "    length = subimage.shape[0]*subimage.shape[1]*subimage.shape[2]\n",
    "    imgVector = subimage.reshape((length))\n",
    "    mean = np.mean(imgVector)\n",
    "    sd = np.std(imgVector)\n",
    "    ratio = round(sd/mean,4)\n",
    "    return(mean,sd,ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c1d2b",
   "metadata": {},
   "source": [
    "## Some final setup\n",
    "\n",
    "Create an output folder (`Train Images_YYYY-MM-DD_hh-mm-ss`) to hold the created sub-images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9251a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to the output folder: Train Images_2025-04-09_13-08-20\n",
      "C:\\Users\\jsluka\\OneDrive - Indiana University\\Desktop\\Work\\Watanabe ovary 2021\\MOTHER\\AIML_code_for_Paper\\Program 1\\Train Images_2025-04-09_13-08-20\n"
     ]
    }
   ],
   "source": [
    "# (Delete then re)create the output dir\n",
    "path = os.getcwd()\n",
    "#path = os.path.join(path,'Train Images')\n",
    "Work_Folder = \"Train Images_\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "print('Writing to the output folder:',Work_Folder)\n",
    "path = os.path.join(path,Work_Folder)\n",
    "'''\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print(\"created new output dir:\\n\",path) \n",
    "else:\n",
    "    #os.rmdir(path)\n",
    "    shutil.rmtree(path)\n",
    "    os.mkdir(path)\n",
    "    print(\"output dir already exists, deleting then creating an empty dir:\\n \",path)\n",
    "'''\n",
    "os.mkdir(path)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e55df5b",
   "metadata": {},
   "source": [
    "## Do the main processing\n",
    "Actually process every file in the file list, and every follicle annotation for every file, and create the subimages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e760097",
   "metadata": {
    "id": "7e760097",
    "outputId": "74dacd97-9637-4842-9635-d5b8b91ae479",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This may take a while, wait for the \"Done\" message ...\n",
      "\n",
      "Doing file 1 of 18 \n",
      "   ./TrainingData_20250409/14736_UN_050a.ome.tif \n",
      "   ./TrainingData_20250409/14736_UN_050a.annotations.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (7403, 6024, 3)\n",
      "      lines in annotations= 75\n",
      "         new dir: Train Images_2025-04-09_13-08-20\\Transitional Primordial\n",
      "         new dir: Train Images_2025-04-09_13-08-20\\Primary\n",
      "         new dir: Train Images_2025-04-09_13-08-20\\Multilayer\n",
      "         new dir: Train Images_2025-04-09_13-08-20\\Primordial\n",
      "         new dir: Train Images_2025-04-09_13-08-20\\Transitional Primary\n",
      "         new dir: Train Images_2025-04-09_13-08-20\\Secondary\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 2 of 18 \n",
      "   ./TrainingData_20250409/16418_UN_140b.ome.tif \n",
      "   ./TrainingData_20250409/16418_UN_140b.annotations.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (9128, 14464, 3)\n",
      "      lines in annotations= 71\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 3 of 18 \n",
      "   ./TrainingData_20250409/19006_UN_020a.ome.tif \n",
      "   ./TrainingData_20250409/19006_UN_020a.annotations.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (8093, 11246, 3)\n",
      "      lines in annotations= 243\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 4 of 18 \n",
      "   ./TrainingData_20250409/25058_LT_005a.ome.tif \n",
      "   ./TrainingData_20250409/25058_LT_005a.annotations.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (11311, 14356, 3)\n",
      "      lines in annotations= 379\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 5 of 18 \n",
      "   ./TrainingData_20250409/25065_LT_010a.ome.tif \n",
      "   ./TrainingData_20250409/25065_LT_010a.annotations.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (12151, 8540, 3)\n",
      "      lines in annotations= 409\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 6 of 18 \n",
      "   ./TrainingData_20250409/25081_LT_010a.ome.tif \n",
      "   ./TrainingData_20250409/25081_LT_010a.annotations.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (10433, 8890, 3)\n",
      "      lines in annotations= 468\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 7 of 18 \n",
      "   ./TrainingData_20250409/27570_UN_110a.ome.tif \n",
      "   ./TrainingData_20250409/27570_UN_110a.annotations.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (9369, 6188, 3)\n",
      "      lines in annotations= 614\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 8 of 18 \n",
      "   ./TrainingData_20250409/30381_RT_070b.ome.tif \n",
      "   ./TrainingData_20250409/30381_RT_070b.ome.annotationsTable.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (10713, 19678, 3)\n",
      "      lines in annotations= 531\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 9 of 18 \n",
      "   ./TrainingData_20250409/30381_RT_140b.ome.tif \n",
      "   ./TrainingData_20250409/30381_RT_140b.ome.annotationsTable.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (10184, 18154, 3)\n",
      "      lines in annotations= 897\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 10 of 18 \n",
      "   ./TrainingData_20250409/30381_RT_200c.ome.tif \n",
      "   ./TrainingData_20250409/30381_RT_200c.ome.annotationsTable.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (7387, 15839, 3)\n",
      "      lines in annotations= 1034\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 11 of 18 \n",
      "   ./TrainingData_20250409/32002_RT_050a.ome.tif \n",
      "   ./TrainingData_20250409/32002_RT_050a.ome.annotationsTable.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (10375, 14815, 3)\n",
      "      lines in annotations= 272\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 12 of 18 \n",
      "   ./TrainingData_20250409/32002_RT_110b.ome.tif \n",
      "   ./TrainingData_20250409/32002_RT_110b.ome.annotationsTable.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (12401, 15198, 3)\n",
      "      lines in annotations= 527\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 13 of 18 \n",
      "   ./TrainingData_20250409/32002_RT_160c.ome.tif \n",
      "   ./TrainingData_20250409/32002_RT_160c.ome.annotationsTable.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (8470, 10814, 3)\n",
      "      lines in annotations= 518\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 14 of 18 \n",
      "   ./TrainingData_20250409/33564_RT_060a.ome.tif \n",
      "   ./TrainingData_20250409/33564_RT_060a.ome.annotationsTable.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (11675, 12860, 3)\n",
      "      lines in annotations= 775\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 15 of 18 \n",
      "   ./TrainingData_20250409/33564_RT_120b.ome.tif \n",
      "   ./TrainingData_20250409/33564_RT_120b.ome.annotationsTable.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (12094, 15396, 3)\n",
      "      lines in annotations= 435\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 16 of 18 \n",
      "   ./TrainingData_20250409/33564_RT_180b.ome.tif \n",
      "   ./TrainingData_20250409/33564_RT_180b.ome.annotationsTable.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (10918, 13189, 3)\n",
      "      lines in annotations= 296\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 17 of 18 \n",
      "   ./TrainingData_20250409/21930_LT_060a.ome.tif \n",
      "   ./TrainingData_20250409/21930_LT_060a_annotationsTable.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (10216, 10085, 3)\n",
      "      lines in annotations= 102\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "Doing file 18 of 18 \n",
      "   ./TrainingData_20250409/21930_LT_120b.ome.tif \n",
      "   ./TrainingData_20250409/21930_LT_120b_annotationsTable.txt \n",
      "   Conv_ratio: 0.69\n",
      "      image dimensions: (12154, 11746, 3)\n",
      "      lines in annotations= 128\n",
      "\tTotal subImageFails: 0 \n",
      "\t [] \n",
      "\n",
      "\n",
      "Done  :)\n"
     ]
    }
   ],
   "source": [
    "print('\\nThis may take a while, wait for the \"Done\" message ...\\n')\n",
    "\n",
    "for k in range(len(parameters.file_list)):\n",
    "    print(\"Doing file\",k+1,\"of\",len(parameters.file_list),\n",
    "          \"\\n  \",parameters.file_list[k][\"Image path\"], \\\n",
    "          \"\\n  \",parameters.file_list[k][\"Annotation path\"], \\\n",
    "          \"\\n   Conv_ratio:\",parameters.file_list[k][\"Resolution\"])\n",
    "    data(parameters.file_list[k][\"Image path\"], parameters.file_list[k][\"Annotation path\"], parameters.file_list[k][\"Resolution\"], width = parameters.types[follicle_type]['window_size'])\n",
    "print(\"\\nDone  :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fa501",
   "metadata": {},
   "source": [
    "## Finish up by getting counts by type and creating some auxilary folders and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e63ceb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work_Folder:\n",
      " C:\\Users\\jsluka\\OneDrive - Indiana University\\Desktop\\Work\\Watanabe ovary 2021\\MOTHER\\AIML_code_for_Paper\\Program 1\\Train Images_2025-04-09_13-08-20\n",
      "Follicle type = Primary\n",
      "Output to:\n",
      "C:\\Users\\jsluka\\OneDrive - Indiana University\\Desktop\\Work\\Watanabe ovary 2021\\MOTHER\\AIML_code_for_Paper\\Program 1\\\n",
      "\n",
      "Counts are number of original annotations plus augmentations. \n",
      "To get original number of annotations, omit \"Negative\" and then divide by 16, or 32 or 48\n",
      "depending on what augmentations are being used.\n",
      "\n",
      "  2752   Multilayer                     \".\\C:\\Users\\jsluka\\OneDrive - Indiana University\\Desktop\\Work\\Watanabe ovary 2021\\MOTHER\\AIML_code_for_Paper\\Program 1\\Train Images_2025-04-09_13-08-20\\Multilayer\" \n",
      "497536   Negative                       \".\\C:\\Users\\jsluka\\OneDrive - Indiana University\\Desktop\\Work\\Watanabe ovary 2021\\MOTHER\\AIML_code_for_Paper\\Program 1\\Train Images_2025-04-09_13-08-20\\Negative\" \n",
      "  3760   Primary                        \".\\C:\\Users\\jsluka\\OneDrive - Indiana University\\Desktop\\Work\\Watanabe ovary 2021\\MOTHER\\AIML_code_for_Paper\\Program 1\\Train Images_2025-04-09_13-08-20\\Primary\" \n",
      " 42576   Primordial                     \".\\C:\\Users\\jsluka\\OneDrive - Indiana University\\Desktop\\Work\\Watanabe ovary 2021\\MOTHER\\AIML_code_for_Paper\\Program 1\\Train Images_2025-04-09_13-08-20\\Primordial\" \n",
      "  1840   Secondary                      \".\\C:\\Users\\jsluka\\OneDrive - Indiana University\\Desktop\\Work\\Watanabe ovary 2021\\MOTHER\\AIML_code_for_Paper\\Program 1\\Train Images_2025-04-09_13-08-20\\Secondary\" \n",
      "  4016   Transitional Primary           \".\\C:\\Users\\jsluka\\OneDrive - Indiana University\\Desktop\\Work\\Watanabe ovary 2021\\MOTHER\\AIML_code_for_Paper\\Program 1\\Train Images_2025-04-09_13-08-20\\Transitional Primary\" \n",
      " 69440   Transitional Primordial        \".\\C:\\Users\\jsluka\\OneDrive - Indiana University\\Desktop\\Work\\Watanabe ovary 2021\\MOTHER\\AIML_code_for_Paper\\Program 1\\Train Images_2025-04-09_13-08-20\\Transitional Primordial\" \n",
      "\n",
      "Total images = 621920\n",
      "Total images w/o \"Negatives\" = 124384\n",
      "Total annotations = 7774.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# collect some statistics and save to the log file\n",
    "print('Work_Folder:\\n',path)\n",
    "Dir_List = glob.glob(path+'/*')\n",
    "\n",
    "Total_Images = 0\n",
    "Total_Images_NN = 0\n",
    "aText = \"Follicle type = \"+follicle_type2+\"\\n\"\n",
    "aText += 'Output to:\\n'+os.path.join(os.getcwd(),\"\")+\"\\n\\n\"\n",
    "aText += 'Counts are number of original annotations plus augmentations. \\n'\n",
    "aText += 'To get original number of annotations, omit \"Negative\" and then divide by 16, or 32 or 48\\n'\n",
    "aText += 'depending on what augmentations are being used.\\n\\n'\n",
    "\n",
    "for aDir in Dir_List:\n",
    "    file_list = glob.glob(os.path.join(aDir,'*.png'))\n",
    "    Total_Images += len(file_list)\n",
    "    if os.path.split(aDir)[1] != 'Negative':\n",
    "        Total_Images_NN += len(file_list)\n",
    "    aText += ('%6i   %-30s \".%s%s\" ' % (len(file_list),os.path.split(aDir)[1],os.sep,aDir))+\"\\n\"\n",
    "    \n",
    "aText += '\\nTotal images = '+str(Total_Images)+'\\n'\n",
    "aText += 'Total images w/o \"Negatives\" = '+str(Total_Images_NN)+'\\n'\n",
    "aText += 'Total annotations = '+str(Total_Images_NN/16)+'\\n'\n",
    "print(aText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6f01989",
   "metadata": {
    "id": "c6f01989",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Work log folder named: 'Project__25-04-09__14_15_40'\n"
     ]
    }
   ],
   "source": [
    "# Creating a work log folder\n",
    "##file_name = parameters.create_folder(follicle_type)\n",
    "file_name = parameters.create_folder(\"Project_\")\n",
    "print('Created Work log folder named: \\'',file_name,'\\'',sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3336d92",
   "metadata": {
    "id": "f3336d92",
    "outputId": "c148ef55-5feb-4489-9efe-e9949986666b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Project__25-04-09__14_15_40/parameters.py'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copying the parameters file into the work log folder\n",
    "shutil.copy('parameters.py', file_name + '/parameters.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2318137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project__25-04-09__14_15_40\\work_log.txt\n"
     ]
    }
   ],
   "source": [
    "# write the summary statistics and other info ino a log file in the work log folder\n",
    "f = open(os.path.join(file_name,\"work_log.txt\"),\"w+\")\n",
    "print(f.name)\n",
    "f.write(file_name+\"\\n\")\n",
    "f.write(aText)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8ebdfd6",
   "metadata": {
    "id": "e8ebdfd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work_log_filename.txt\n"
     ]
    }
   ],
   "source": [
    "# Creating a text file to store the name of the work log file\n",
    "f = open(\"work_log_filename.txt\",\"w+\")\n",
    "print(f.name)\n",
    "f.write(file_name)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1847f9e5",
   "metadata": {},
   "source": [
    "## Convert Notebook to HTML and save\n",
    "\n",
    "Programmatically save the notebook, convert it to html, rename the .html file with a timestamp.\n",
    "\n",
    "Make sure the NOTEBOOK_NAME and NOTEBOOK_HTML_NAME are properly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe0e3b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTEBOOK_NAME: Train Image Generation v5_3.ipynb\n",
      "NOTEBOOK_HTML_NAME: Train Image Generation v5_3_2025-04-09_14-15-52.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Train Image Generation v5_3.ipynb to html\n",
      "[NbConvertApp] Writing 732712 bytes to Train Image Generation v5_3.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Train Image Generation v5_3_2025-04-09_14-15-52.html'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Needed for the next command to work, for some reason\n",
    "time.sleep(1)\n",
    "\n",
    "#Programmatically save the notebook\n",
    "\n",
    "APP = JupyterFrontEnd() #Needed to save the notebook programmatically later, do not change.\n",
    "APP.commands.execute(\"docmanager:save\")\n",
    "\n",
    "#Convert the notebook to html\n",
    "NOTEBOOK_NAME = \"Train Image Generation v5_3.ipynb\" #The exact name of this notebook, including the file extension.\n",
    "print('NOTEBOOK_NAME:',NOTEBOOK_NAME)\n",
    "!jupyter nbconvert --to html \"$NOTEBOOK_NAME\"\n",
    "\n",
    "#Rename the .html file with a timestamp\n",
    "NOTEBOOK_HTML_NAME = \"Train Image Generation v5_3_\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".html\"\n",
    "print('NOTEBOOK_HTML_NAME:',NOTEBOOK_HTML_NAME)\n",
    "shutil.move(NOTEBOOK_NAME[:-6] + \".html\",NOTEBOOK_HTML_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d522def",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
