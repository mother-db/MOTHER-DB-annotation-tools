{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0bc8b6e-0ac1-4dff-b0b3-f5867b312edb",
   "metadata": {},
   "source": [
    "# Program 4\n",
    "Adapted from previous iteration by Parth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0874153-77f6-4bbb-b34e-216ba7f6d29b",
   "metadata": {},
   "source": [
    "## Base Setup\n",
    "\n",
    "This section contains the basic environment set up for this notebook, including imports, constants, and any variable that needs to be easily accessed for changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d88c3f-e501-4baa-af8b-ec9fb8a7f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import gc\n",
    "import platform\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from ast import literal_eval\n",
    "from ipylab import JupyterFrontEnd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95642f-0cd1-4603-89d2-4797cd6ac467",
   "metadata": {},
   "source": [
    "This is a set of constants used mainly for workspace setup.\n",
    "\n",
    "`CHECKPOINT`: The base name of the checkpoint file(s) being used to make predictions on the sample image(s). Current version of the program expects \"epoch_\" or some other similar 6 characters at the end when not using multiple epochs for predictions.\\\n",
    "`OUTPUT_DIR`: The absolute path from the current directory to the directory to be used for output files made by this notebook. <b>Note:</b> The directory structure may already exist, but it does not need to. A later function will make it if it does not exist.\\\n",
    "`SAMPLE_DIR`: The relative or absolute path to the directory containing the sample image(s) having predictions made on them.\\\n",
    "`SAMPLE_IMAGE_DICT`: A dictionary containing keys, which are the complete names of the sample image files, and their corresponding values, which are the complete names of the annotation files associated with those sample images. Currently, used mainly for testing purposes.\\\n",
    "`COLOR_PALETTE`: A list containing colors to be used when visually plotting the prediction heatmaps. Each class has its own color.\\\n",
    "`NOTEBOOK_NAME`: The exact name of this notebook, including the file extension. Needed later for programmatic html conversion and copying of the notebook.\\\n",
    "`SAVED_FILES`: Not technically a constant, but should not be altered by user. Used to keep track of any non-checkpoint file that gets saved to later move to output.\\\n",
    "`RUNTIMES`: Not technivally a constant, but should not be altered by use. Used to track the prediction and total time it takes for each image being processed.\\\n",
    "`APP`: JupyterFrontEnd instance that is used to save the notebook programmatically later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11a895-c35b-46d1-bd3c-9f697f4edc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define constants\n",
    "CHECKPOINT = \"ResNet34_Small_Follicles_Epoch_\"\n",
    "OUTPUT_DIR = \"Output/Small Follicles/ResNet34/Run Final 1/\"\n",
    "SAMPLE_DIR = \"../../Data/Original/\"\n",
    "SAMPLE_IMAGE_DICT = {\n",
    "    \"14736_UN_050a.ome.tif\": \"14736_UN_050a.annotations.txt\",\n",
    "    \"16418_UN_140b.ome.tif\": \"16418_UN_140b.annotations.txt\",\n",
    "    \"19006_UN_020a.ome.tif\": \"19006_UN_020a.annotations.txt\",\n",
    "    \"21930_LT_060a.ome.tif\": \"21930_LT_060a_annotationsTable.txt\",\n",
    "    \"21930_LT_120b.ome.tif\": \"21930_LT_120b_annotationsTable.txt\",\n",
    "    \"25058_LT_005a.ome.tif\": \"25058_LT_005a.annotations.txt\",\n",
    "    \"25065_LT_010a.ome.tif\": \"25065_LT_010a.annotations.txt\",\n",
    "    \"25081_LT_010a.ome.tif\": \"25081_LT_010a.annotations.txt\",\n",
    "    \"27570_UN_110a.ome.tif\": \"27570_UN_110a.annotations.txt\",\n",
    "    \"30381_RT_070b.ome.tif\": \"30381_RT_070b.ome.annotationsTable.txt\",\n",
    "    \"30381_RT_140b.ome.tif\": \"30381_RT_140b.ome.annotationsTable.txt\",\n",
    "    \"30381_RT_200c.ome.tif\": \"30381_RT_200c.ome.annotationsTable.txt\",\n",
    "    \"32002_RT_050a.ome.tif\": \"32002_RT_050a.ome.annotationsTable.txt\",\n",
    "    \"32002_RT_110b.ome.tif\": \"32002_RT_110b.ome.annotationsTable.txt\",\n",
    "    \"32002_RT_160c.ome.tif\": \"32002_RT_160c.ome.annotationsTable.txt\",\n",
    "    \"33564_RT_060a.ome.tif\": \"33564_RT_060a.ome.annotationsTable.txt\",\n",
    "    \"33564_RT_120b.ome.tif\": \"33564_RT_120b.ome.annotationsTable.txt\",\n",
    "    \"33564_RT_180b.ome.tif\": \"33564_RT_180b.ome.annotationsTable.txt\",\n",
    "    \"DP28_25081_Section3_10X_ome_copy.tif\": \"DP28_25081_Section3_10X_ome_copy.annotations.txt\",\n",
    "    \"32002_LT_180a.ome.tif\": \"32002_LT_180a.ome.annotationsTable.txt\",\n",
    "    \"KY_PS_LB40SDwk16601_7_a.ome.tif\": \"LB40_SDwk16601_7a_annotationsTable.txt\"\n",
    "}\n",
    "COLOR_PALETTE = ['white', 'red', 'gold', 'blue', 'green', 'darkviolet', 'dimgray']\n",
    "NOTEBOOK_NAME = \"Program 4 - Final v2.ipynb\" #Make sure this is identical to the name of THIS notebook\n",
    "SAVED_FILES = [] #Leave as an empty list\n",
    "RUNTIMES = {} #Leave as an empty dictionary\n",
    "APP = JupyterFrontEnd() #Needed to save the notebook programmatically later, do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827ba2f-3564-4f57-b55e-9dbc62a28224",
   "metadata": {},
   "source": [
    "These are variables and flags for functions that get used later.\n",
    "\n",
    "`transform`: The set of torchvision transforms to be applied to the images before they are input into the model before prediction. <b>Note:</b> This variable should be identical to the one in Program 3 that was used to train the model, otherwise the predictions will be very incorrect.\\\n",
    "`batch_size`: The amount of images per DataLoader batch. Heavily affects VRAM usage. Speed testing has determined 128 to be optimal for my hardware.\\\n",
    "`num_workers`: The amount of workers for the DataLoader to use to parallelize training. Affects system RAM usage. Speed testing has determined 8 to be optimal for my hardware. <b>Note:</b> Windows is incapable of parallelizing Jupyter Notebooks like this; therefore, this variable will be set to 0 if on Windows.\\\n",
    "`freeze_model`: Flag to determine whether or not to freeze all layers of the model except the final layer. Testing has shown better prediction performance with this set to False. <b>Note:</b> This should be set to the same value as was used in Program 3 to train the model.\\\n",
    "`num_classes`: The number of output classes to be added to the new final layer of the model. <b>Note:</b> This should be the same as the number of output classes used in Program 3 when training the model.\\\n",
    "`do_entire_image`: Flag to determine whether to make predictions over an entire image or a small section. <b>Warning:</b> If this is False, any image that will be predicted on needs to have both a row and collumn slice. The program will throw a reminder error if this is not done.\\\n",
    "`row_slices`: Dictionary whose keys are sample image file names, like `SAMPLE_IMAGE_DICT`. The values are lists containing ranges that denote the slices of rows to be predicted on for a particular slice on the image in the key. Only used if `do_entire_image` is False. <b>Note:</b> Matches rows to collumns based on location in the list.\\\n",
    "`col_slices`: Dictionary whose keys are sample image file names, like `SAMPLE_IMAGE_DICT`. The values are lists containing ranges that denote the slices of collumns to be predicted on for a particular slice on the image in the key. Only used if `do_entire_image` is False. <b>Note:</b> Matches collumns to rows based on location in the list.\\\n",
    "`do_certain_images`: Flag to determine whether to do every image contained in `SAMPLE_IMAGE_DICT`. If True, only images in `images_to_do` will have predictions made. Otherwise, all sample images are done.\\\n",
    "`images_to_do`: A list containing the full sample image file names as strings of every sample image you want to be predicted on. <b>Warning:</b> If `do_entire_image` is False and `do_certain_images` is True, any image in this list must have row and collumn slices denoted in `row_slices` and `col_slices` respectively.\\\n",
    "`window_size`: The size of one side of the square window to use for predictions. <b>Note:</b> Default is 200. Should be the same number used in Program 1 to generate the training images.\\\n",
    "`window_radius`: The window radius to use when making predictions. Half of the `window_size`. <b>Note:</b> If the next variable, `image_scaling_testing` is `True`, this value will be multiplied by the value in `multiplier` as well.\\\n",
    "`image_scaling_testing`: Flag to determine whether to scale the image when making predictions. Testing has found that scaling an image down speeds up prediction times at the cost of accuracy due to the data loss involved with lowering resolution. <b>Deprecated:</b> Pixel skip does better in both time and accuracy metrics. This functionality may or may not even work.\\\n",
    "`multiplier`: The multiplier to use when scaling an image. >1 for up-scaling. >0 and <1 for down-scaling.\\\n",
    "`enable_pixel_skip`: Flag to determine whether to use pixel-skipping when making predictions. Testing has found it significantly speeds up prediction times with minimal cost to accuracy as long as `pixel_skip` is small.\\\n",
    "`pixel_skip`: The number of pixels to skip both horizontally and vertically. 5 is what we currently use. <b>Warning:</b> Large numbers will lose prediction accuracy as entire smaller follicles could be missed and prediction blobs become blockier.\\\n",
    "`save_figs`: Flag to determine whether to save any extra figures created in this notebook for analysis purposes.\\\n",
    "`save_dataframes`: Flag to determine whether to save the prediction dataframes that are created in this program. <b>Note:</b> Requires `save_figs` to be True for this to have an effect.\\\n",
    "`use_amp`: Flag to determine whether or not to use PyTorch's Automatic Mixed Precision. Can significantly boost performance with minimal cost to calculation accuracy.\\\n",
    "`multiple_epochs`: Flag to determine if predictions are going to be made using checkpoints from every epoch generated by Program 3 when its `save_each_epoch` flag is True.\\\n",
    "`num_epochs`: The number of epochs that Program 3 was run with if the `save_each_epoch` flag was True. Only used when `multiple_epochs` is True.\\\n",
    "`checkpoint_name`: The unique addition to the end of `CHECKPOINT` to know the checkpoint file to use. Currently, the program expects \"epoch_\" or another 6 characters to be at the end of `CHECKPOINT` to be replaced by this variable. Only used when `multiple_epochs` is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667501ad-51ae-4a09-80f5-5c1cae2b5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define variables\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 0 if platform.system() == \"Windows\" else 8\n",
    "\n",
    "freeze_model = False\n",
    "num_classes = 7\n",
    "\n",
    "do_entire_image = True\n",
    "row_slices = {\n",
    "    \"14736_UN_050a.ome.tif\": [range(1000, 2000)],\n",
    "    \"27570_UN_110a.ome.tif\": [range(500, 1500)],\n",
    "    \"32002_LT_180a.ome.tif\": [range(1500, 2500), range(3300, 4300)],\n",
    "    \"KY_PS_LB40SDwk16601_7_a.ome.tif\": []\n",
    "}\n",
    "col_slices = {\n",
    "    \"14736_UN_050a.ome.tif\": [range(1500, 2500)],\n",
    "    \"27570_UN_110a.ome.tif\": [range(2000, 3000)],\n",
    "    \"32002_LT_180a.ome.tif\": [range(5500, 6500), range(8000, 9000)],\n",
    "    \"KY_PS_LB40SDwk16601_7_a.ome.tif\": []\n",
    "}\n",
    "\n",
    "do_certain_images = True\n",
    "images_to_do = [\"14736_UN_050a.ome.tif\", \"DP28_25081_Section3_10X_ome_copy.tif\"]\n",
    "\n",
    "window_size = 200\n",
    "window_radius = int(window_size / 2)\n",
    "\n",
    "image_scaling_testing = False\n",
    "multiplier = 0.5\n",
    "if image_scaling_testing:\n",
    "    window_radius = int(window_radius * multiplier)\n",
    "\n",
    "enable_pixel_skip = True\n",
    "pixel_skip = 5\n",
    "\n",
    "save_figs = True\n",
    "save_dataframes = True\n",
    "\n",
    "use_amp = True\n",
    "\n",
    "multiple_epochs = True\n",
    "num_epochs = 25\n",
    "checkpoint_name = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff792723-bbe8-4d24-b2d6-66b3fe5ce89d",
   "metadata": {},
   "source": [
    "### Class and Function Definitions\n",
    "\n",
    "This section contains all the Classes and Functions used by this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9f400e-09cc-45a8-b904-6bf86c53a7d2",
   "metadata": {},
   "source": [
    "`read_image`: Loads an image file and makes any modifications, if necessary.\n",
    "\n",
    "<b>Parameters:</b>\\\n",
    "&emsp;`file`: The image file to be loaded in.\\\n",
    "&emsp;`image_slice`: A dictionary containing the row and collumn slices to modify the image with. <b>Default:</b> None\n",
    "\n",
    "Loads in the image using `cv2` and converts it to the correct color layout.\\\n",
    "If `image_scaling_testing` is True, resizes the image based on `multiplier`.\\\n",
    "If `image_slice` parameter was passed a value, create a slice of the loaded image.\n",
    "\n",
    "<b>Returns:</b>\\\n",
    "&emsp;The loaded image with any potential modifications made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34699ed5-783c-4567-aafa-2e3ae0cab0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file, image_slice = None):\n",
    "    '''Read in the given image, file, then modify as requested. If image_slice is passed a dictionary containing row and col range objects,\n",
    "    it will be used to create a slice of the image being read in.'''\n",
    "    image = cv2.cvtColor(cv2.imread(SAMPLE_DIR + file), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if image_scaling_testing:\n",
    "        image = cv2.resize(image, dsize = None, fx = multiplier, fy = multiplier, interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "    if image_slice is not None:\n",
    "        image = image[image_slice['row'][0]:image_slice['row'][-1] + 1, image_slice['col'][0]:image_slice['col'][-1] + 1, :]\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dabf3a-3c46-4bbe-8ca1-ac6b4ea40c4a",
   "metadata": {},
   "source": [
    "`read_probability_csv`: Reads in a class probability csv file. Mainly used for running analyses later without having to remake the predictions. <b>Deprecated:</b> After switching from csv files to parquet files, this is no longer needed as parquet files are binary and thus don't lose the data types when being saved and loaded.\n",
    "\n",
    "<b>Parameters:</b>\\\n",
    "&emsp;`file`: The probabilities csv file to be loaded.\n",
    "\n",
    "Reads in the csv file. Then, processes each item in the dataframe to remake the original list that was converted to a string when saving.\n",
    "\n",
    "<b>Returns:</b>\\\n",
    "&emsp;The newly loaded dataframe containing lists of probabilities at each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f670d-b3d1-42f2-a4d5-449042023143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_probability_csv(file):\n",
    "    '''Read in the class probabilities csv, if necessary. Everything will be strings, so convert to proper data type.\n",
    "    Returns the opened DataFrame.'''\n",
    "    #Read in csv file\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    #Convert string values to lists\n",
    "    for row in df.index:\n",
    "        for col in df.columns:\n",
    "            df.loc[row, col] = literal_eval(re.sub(\"\\s+\", ',', df.loc[row, col]))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02239961-ad73-4e9f-b799-531f6591ce84",
   "metadata": {},
   "source": [
    "`setup_model`: Prepares the image classification model being used.\n",
    "\n",
    "<b>Parameters:</b>\\\n",
    "&emsp;`model`: The image classification model being setup.\n",
    "\n",
    "If `freeze_model` flag is True, loops through the existing layers of the model and freezes them.\\\n",
    "Then, replaces the fully connected layer of the model with a Linear layer that has a number of output features equal to `num_classes`.\\\n",
    "Finally, sends the model to the Torch device `device`.\n",
    "\n",
    "<b>Returns:</b>\\\n",
    "&emsp;The model set to eval mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070844d8-0cb3-4605-9424-6bfa48158a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(model):\n",
    "    '''Applies desired modifications to given model. Returns model in eval mode.'''\n",
    "    #Freeze model, if desired\n",
    "    if freeze_model:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    #Replace final layer\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    #Send model to torch device\n",
    "    model.to(device)\n",
    "\n",
    "    return model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15284c63-674d-4812-b480-53cf34316ace",
   "metadata": {},
   "source": [
    "`check_coord_bounds`: Checks if the coordinates contained in a dataframe are within certain bounds.\n",
    "\n",
    "<b>Parameters:</b>\\\n",
    "&emsp;`df`: The dataframe containing coordinates.\\\n",
    "&emsp;`x`: A range object of the x bound.\\\n",
    "&emsp;`y`: A range object of the y bound.\\\n",
    "&emsp;`row_num`: The row number of the coordinates in the dataframe that are currently being looked at.\n",
    "\n",
    "<b>Returns:</b>\\\n",
    "&emsp;A boolean of whether the coordinates in the desired row of the dataframe are within the x, y bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c8640-aaa4-41fb-a8c2-87504f99c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coord_bounds(df, x, y, row_num):\n",
    "    '''Checks whether coordinates in a given row of a given dataframe are within the given x, y bounds.'''\n",
    "    return x[0] <= df['Centroid Y px'][row_num] <= x[-1] and y[0] <= df['Centroid X px'][row_num] <= y[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b708f7e-1450-42d0-87b3-4f12d8b67dbb",
   "metadata": {},
   "source": [
    "`get_coords`: Gets the modified coordinates contained in a dataframe as a list.\n",
    "\n",
    "<b>Parameters:</b>\\\n",
    "&emsp;`df`: The dataframe containing coordinates.\\\n",
    "&emsp;`x`: A range object of the x bound.\\\n",
    "&emsp;`y`: A range object of the y bound.\\\n",
    "&emsp;`row_num`: The row number of the coordinates in the dataframe that are currently being looked at.\n",
    "\n",
    "<b>Returns:</b>\\\n",
    "&emsp;A list containing the coordinates in a row of the dataframe that have been modified to fit in the x and y range bounds where the beginning of those bounds are the new 0, 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed12f66-5536-4c89-93a6-d585ff6f66ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(df, x, y, row_num):\n",
    "    '''Returns a list of coords from a given row of a given dataframe with the given x, y bounds subtracted to fit on a plot.'''\n",
    "    return list(df[['Centroid X px', 'Centroid Y px']].iloc[row_num].to_numpy() - np.array([y[0], x[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da9184-9d30-48e9-b6a6-45dbba8b0aa0",
   "metadata": {},
   "source": [
    "`ImageSet`: Class that extends `torchvision.datasets.VisionDataset`\n",
    "\n",
    "This class extends the base VisionDataset class in torchvision to create a dataset that can store the image slices of the full image to have predictions made.\n",
    "\n",
    "<b>Methods:</b>\\\n",
    "&emsp;`__init__`: Initializes the dataset. Override.\n",
    "\n",
    "&emsp;<b>Parameters:</b>\\\n",
    "&emsp;&emsp;`image_file`: The file path for the full image that will be having predictions made on it.\\\n",
    "&emsp;&emsp;`transforms`: The torchvision transforms to apply to the subimage before it is sent to the model.\\\n",
    "&emsp;&emsp;`centers`: A list of tuples containing the valid center points for subimages.\\\n",
    "&emsp;&emsp;`radius`: The window radius used to cut out a subimage.\n",
    "\n",
    "&emsp;Calls `read_image` to load in the full image to `self.full_image`.\\\n",
    "&emsp;Stores the center points in `self.images`.\\\n",
    "&emsp;Stores the radius in `self.radius`.\\\n",
    "&emsp;Finally, calls `super().__init__()` passing the `transforms` parameter to finish setting up the dataset.\n",
    "\n",
    "&emsp;------------------------------------------------------------\n",
    "\n",
    "&emsp;`__getitem__`: Gets the item in the dataset at an index. Override.\n",
    "\n",
    "&emsp;<b>Parameters:</b>\\\n",
    "&emsp;&emsp;`index`: An index value to be used to get an item in the dataset.\n",
    "\n",
    "&emsp;Gets the row and collumn center point located at `index` in `self.images`.\n",
    "&emsp;Creates a slice of `self.full_image` using the center point and the `self.radius` value to create the subimage.\n",
    "\n",
    "&emsp;<b>Returns:</b>\\\n",
    "&emsp;&emsp;A tuple containing the center point of the subimage at `index` as well as the transformed subimage which has been cut out of the full image using `self.radius`.\n",
    "\n",
    "&emsp;------------------------------------------------------------\n",
    "\n",
    "&emsp;`__len__`: Gets the length of the dataset. Override.\n",
    "\n",
    "&emsp;<b>Returns:</b>\\\n",
    "&emsp;&emsp;The length of `self.images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e45f46-8c9a-4297-8c4f-746234845704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSet(VisionDataset):\n",
    "    '''Image dataset class extending torchvision.datasets.VisionDataset'''\n",
    "    def __init__(self, image_file, transforms, centers, radius):\n",
    "        '''__init__ override to handle some custom variables. Calls __init__ of parent class to finish setup.'''\n",
    "        self.full_image = read_image(image_file)\n",
    "        self.images = centers\n",
    "        self.radius = radius\n",
    "\n",
    "        super().__init__(transforms = transforms)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''Takes an index and gets the values of the dataset at that index. Returns the x and y values unchanged and transformed image.'''\n",
    "        row, col = self.images[index]\n",
    "        image = self.full_image[row - self.radius:row + self.radius, col - self.radius:col + self.radius, :]\n",
    "        return (row, col, self.transforms(Image.fromarray(image)))\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Gets the length of the dataset.'''\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438bb39-2649-4069-bd62-8abce1442fe7",
   "metadata": {},
   "source": [
    "`make_predictions`: Makes predictions on a given slice of an image.\n",
    "\n",
    "<b>Parameters:</b>\\\n",
    "&emsp;`row`: Range object for slice of rows to be predicted on.\\\n",
    "&emsp;`col`: Range object for slice of collumns to be predicted on.\\\n",
    "&emsp;`image_file`: The file path to the image to be predicted on.\n",
    "\n",
    "Sets up empty numpy arrays to store predictions and probability data.\\\n",
    "If `enable_pixel_skip` is True, sets up variables needed to handle pixel skipping.\\\n",
    "Loops through ranges to create a list containing all the subimage centers that will need to have predictions made.\\\n",
    "Creates dataset and dataloader from list of centers.\\\n",
    "Makes predictions on the subimages, and stores the prediction and probability values into the numpy arrays created earlier. If `enable_pixel_skip` is True, the data is stored in a `pixel_skip` by `pixel_skip` box that is centered on the center value that was just predicted on. Otherwise, the data is stored in its respective pixel.\\\n",
    "Converts numpy arrays to pandas dataframes and saves dataframes if `save_figs` and `save_dataframes` are both True.\n",
    "\n",
    "<b>Returns:</b>\\\n",
    "&emsp;Dataframe of the probabilities for each class at each pixel in the prediction range, dataframe of the predicted class value at each pixel in the prediction range, and the time when predictions were finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b216d3-9a38-414c-98a3-776112c4f6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(row, col, image_file):\n",
    "    '''Iterates over and makes predictions on given image.'''\n",
    "    global SAVED_FILES\n",
    "    \n",
    "    x_cols = len(row)\n",
    "    y_rows = len(col)\n",
    "    class_probability = np.empty((x_cols, y_rows), dtype = object)\n",
    "    prediction = np.zeros((x_cols, y_rows))\n",
    "    radius = window_radius\n",
    "\n",
    "    #Setup for pixel skipping\n",
    "    if enable_pixel_skip:\n",
    "        padding = pixel_skip // 2\n",
    "        \n",
    "        row_first, row_last = row[0] + padding, row[-1]\n",
    "        col_first, col_last = col[0] + padding, col[-1]\n",
    "        \n",
    "        row = range(row_first, row_last + 1, pixel_skip)\n",
    "        col = range(col_first, col_last + 1, pixel_skip)\n",
    "\n",
    "        row_final = row[-1]\n",
    "        col_final = col[-1]\n",
    "\n",
    "        padding_final = [row_last - row_final, col_last - col_final]\n",
    "\n",
    "    #Create list of valid image centers\n",
    "    imgs = []\n",
    "    \n",
    "    for i in row:\n",
    "        for j in col:\n",
    "            imgs.append((i, j))\n",
    "\n",
    "    #Create image dataset and dataloader\n",
    "    imgs = ImageSet(image_file = image_file, transforms = transform, centers = imgs, radius = radius)\n",
    "    imgs_data_loader = DataLoader(imgs, batch_size = batch_size, shuffle = False, num_workers = num_workers, pin_memory = True)\n",
    "\n",
    "    #Make predictions\n",
    "    for row_inds, col_inds, images in tqdm(imgs_data_loader):\n",
    "        with torch.set_grad_enabled(False):\n",
    "            with torch.autocast(device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\", dtype = torch.float16, enabled = use_amp):\n",
    "                outputs = model(images.to(device))\n",
    "\n",
    "        for row_ind, col_ind, output in zip(row_inds.numpy(), col_inds.numpy(), outputs):\n",
    "            if enable_pixel_skip: #Skipping pixels requires more logic, still much faster overall\n",
    "                output_probability = (torch.exp(output) / torch.sum(torch.exp(output)).reshape(-1, 1))[0].data.cpu().numpy()\n",
    "                output_prediction = output.argmax().item()\n",
    "\n",
    "                if row_ind == row_final and col_ind == col_final: #If at both edges\n",
    "                    for x in range(row_ind - row[0] - padding, row_ind - row[0] + padding_final[0] + 1):\n",
    "                        for y in range(col_ind - col[0] - padding, col_ind - col[0] + padding_final[1] + 1):\n",
    "                            class_probability[x, y] = output_probability\n",
    "                            prediction[x, y] = output_prediction\n",
    "                elif row_ind == row_final and col_ind != col_final: #If at bottom edge but not right edge\n",
    "                    for x in range(row_ind - row[0] - padding, row_ind - row[0] + padding_final[0] + 1):\n",
    "                        for y in range(col_ind - col[0] - padding, col_ind - col[0] + padding + 1):\n",
    "                            class_probability[x, y] = output_probability\n",
    "                            prediction[x, y] = output_prediction\n",
    "                elif row_ind != row_final and col_ind == col_final: #If at right edge but not bottom edge\n",
    "                    for x in range(row_ind - row[0] - padding, row_ind - row[0] + padding + 1):\n",
    "                        for y in range(col_ind - col[0] - padding, col_ind - col[0] + padding_final[1] + 1):\n",
    "                            class_probability[x, y] = output_probability\n",
    "                            prediction[x, y] = output_prediction\n",
    "                else: #If not at either edge\n",
    "                    for x in range(row_ind - row[0] - padding, row_ind - row[0] + padding + 1):\n",
    "                        for y in range(col_ind - col[0] - padding, col_ind - col[0] + padding + 1):\n",
    "                            class_probability[x, y] = output_probability\n",
    "                            prediction[x, y] = output_prediction\n",
    "            else: #Not skipping pixels\n",
    "                class_probability[row_ind - row[0], col_ind - col[0]] = (torch.exp(output) / torch.sum(torch.exp(output)).reshape(-1, 1))[0].data.cpu().numpy()\n",
    "                prediction[row_ind - row[0], col_ind - col[0]] = output.argmax().item()\n",
    "                \n",
    "    #Memory management\n",
    "    del imgs, imgs_data_loader\n",
    "    \n",
    "    #Get time for finished predictions\n",
    "    pred_time = datetime.datetime.now()\n",
    "    \n",
    "    #Convert to dataframes\n",
    "    class_probability = pd.DataFrame(class_probability)\n",
    "    prediction = pd.DataFrame(prediction)\n",
    "    \n",
    "    #Save dataframes\n",
    "    if save_figs and save_dataframes:\n",
    "        files = [\"Class_Probabilities.parquet\", \"Predictions.parquet\"]\n",
    "\n",
    "        #Keep track of files\n",
    "        for file in files:\n",
    "            SAVED_FILES.append(file)\n",
    "        \n",
    "        class_probability.to_parquet(files[0], index = False)\n",
    "        prediction.to_parquet(files[1], index = False)\n",
    "        \n",
    "    return class_probability, prediction, pred_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f84de-e34c-4c0a-9c14-8d614beb5853",
   "metadata": {},
   "source": [
    "`make_output_dir`: Create output directory structure to store files created by this program.\n",
    "\n",
    "<b>Parameters:</b>\\\n",
    "&emsp;`slice_num`: The slice number that is currently being predicted on for a particular image.\\\n",
    "&emsp;`image_file`: The file name of the image that is currently being predicted on.\\\n",
    "&emsp;`epoch`: The epoch number of the checkpoint that is currently being used to make predictions. <b>Default:</b> None.\n",
    "\n",
    "Makes the base output directory if it does not already exist.\\\n",
    "If `epoch` is passed a value, creates a directory within the base output directory for that epoch number. Otherwise, moves to the next step.\\\n",
    "If `do_entire_image` is True, creates a directory within the epoch directory that is just the image name. Otherwise, the new directory contains the image name and the slice number that is currently being predicted on.\\\n",
    "Creates a time-stamped directory within the previously created directory and saves it to a global constant `TIME_STAMP_OUTPUT_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80955f95-4416-43cc-ac29-66fd762dc340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output_dir(slice_num, image_file, epoch = None):\n",
    "    '''Check if the directory specified by OUTPUT_DIR exists. Create directory if it does not exist.\n",
    "    Also create time-stamped directory within.'''\n",
    "    time = datetime.datetime.now()\n",
    "\n",
    "    #Create base output directory if it does not exist\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    #Create epoch directory if passed a value\n",
    "    if epoch is not None:\n",
    "        epoch_dir = OUTPUT_DIR + \"Epoch \" + str(epoch) + \"/\"\n",
    "\n",
    "        if not os.path.exists(epoch_dir):\n",
    "            os.mkdir(epoch_dir)\n",
    "    else:\n",
    "        epoch_dir = OUTPUT_DIR\n",
    "\n",
    "    #Create output directory for image/slice if it does not exist\n",
    "    if do_entire_image:\n",
    "        image_dir = epoch_dir + image_file.split(\".\")[0] + \"/\"\n",
    "    else:\n",
    "        image_dir = epoch_dir + image_file.split(\".\")[0] + \"_slice_\" + str(slice_num + 1) + \"/\"\n",
    "        \n",
    "    if not os.path.exists(image_dir):\n",
    "        os.mkdir(image_dir)\n",
    "\n",
    "    #Define global scope constant\n",
    "    global TIME_STAMP_OUTPUT_DIR\n",
    "    TIME_STAMP_OUTPUT_DIR = image_dir + time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    #Make time-stamped output directory\n",
    "    os.mkdir(TIME_STAMP_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561949be-3e72-45b9-b5db-025f70aacde8",
   "metadata": {},
   "source": [
    "`create_overlay`: Creates and adds an overlay showing where the model made predictions to the original image that was predicted on.\n",
    "\n",
    "<b>Parameters:</b>\\\n",
    "&emsp;`predictions`: The dataframe containing prediction values for each pixel in the slice of the image that was predicted on.\\\n",
    "&emsp;`image_file`: The file name of the image that was predicted on.\\\n",
    "&emsp;`image_slice`: The slice of the image that was predicted on.\n",
    "\n",
    "Defines a function to convert the prediction values to a binary overlay where white means a non-negative prediction was made and black means a negative prediction was made.\\\n",
    "Uses pandas dataframe map method to apply the previously created function to every datapoint in the dataframe.\\\n",
    "Merge the overlay with the original image to make a new image that shows where on the original image non-negative predictions were made.\\\n",
    "If `save_figs` is True, save the newly created image.\n",
    "\n",
    "<b>Returns:</b>\\\n",
    "&emsp;The newly created overlay image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370840aa-70ba-454f-a3e2-a9b7e9a8e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overlay(predictions, image_file, image_slice):\n",
    "    '''Converts predictions into an overlay to be superimposed on the original image slice.'''\n",
    "    def overlay_conversion(value):\n",
    "        '''Return 255 if a non-negative prediction was made for that pixel. Return 0 if a negative prediction was made.'''\n",
    "        if value != 0.0:\n",
    "            return 255\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    #Map overlay_conversion to the prediction dataframe that was passed\n",
    "    overlay = predictions.map(overlay_conversion)\n",
    "\n",
    "    #Memory management\n",
    "    del predictions\n",
    "    \n",
    "    #Merge the overlay into the original image\n",
    "    overlay = cv2.cvtColor(np.asarray(overlay, np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "    overlay = cv2.addWeighted(read_image(image_file, image_slice = image_slice), 0.7, overlay, 0.3, 0.0)\n",
    "    \n",
    "    #Save overlay image\n",
    "    if save_figs:\n",
    "        file = \"Overlay.png\"\n",
    "\n",
    "        #Keep track of file\n",
    "        SAVED_FILES.append(file)\n",
    "\n",
    "        cv2.imwrite(file, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    return overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab8632-debf-422d-9c48-ca7f1c5d6627",
   "metadata": {},
   "source": [
    "`create_probability_data`: Converts the probability dataframe to just contain the maximum probability at each pixel, regardless of which class was predicted.\n",
    "\n",
    "<b>Parameters:</b>\\\n",
    "&emsp;`probabilities`: The dataframe containing each class probability across every pixel.\n",
    "\n",
    "Uses the pandas dataframe map method to apply the max function to every list in the dataframe.\\\n",
    "If `save_figs` is True, saves the new max probabilities dataframe.\n",
    "\n",
    "<b>Returns:</b>\\\n",
    "&emsp;The newly created dataframe that contains only the max probability at each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f57db6f-7dc9-4fc2-a989-c2f7d686909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_probability_data(probabilities):\n",
    "    '''Takes probabilities DataFrame and creates a list of the max data points for each row, col value.\n",
    "    Returns the probability data for graphing.'''\n",
    "    global SAVED_FILES\n",
    "    \n",
    "    #Map max to the entire dataframe\n",
    "    max_probabilities = probabilities.map(max)\n",
    "\n",
    "    #Memory management\n",
    "    del probabilities\n",
    "\n",
    "    #Save max dataframe\n",
    "    if save_figs:\n",
    "        file = \"Max_Probabilities.parquet\"\n",
    "\n",
    "        #Keep track of file\n",
    "        SAVED_FILES.append(file)\n",
    "\n",
    "        max_probabilities.to_parquet(file, index = False)\n",
    "    \n",
    "    return max_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a62f40-ac8a-4010-bd26-2a74cafdcb98",
   "metadata": {},
   "source": [
    "`plot_annotations`: Plots the given annotation coordinates on a given plot.\n",
    "\n",
    "<b>Parameters:</b>\\\n",
    "&emsp;`annotations`: Dictionary containing the coordinates of human annotated follicles with what class they are as the key.\\\n",
    "&emsp;`ax`: The matplotlib axis object to plot the coordinates on.\\\n",
    "&emsp;`marker_size`: A number to determine how large the 'x' markers will be on the plot.\n",
    "\n",
    "Loops through each key, value pair in the `annotations` dictionary and plots the coordinate pairs on the given axis. Each scatter plot is given a label based on the class of follicle it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e10cd5-ac97-474b-821c-4151adb89710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_annotations(annotations, ax, marker_size):\n",
    "    '''Plot the given annotations as a scatterplot on the axis, ax, of an already generated plot.'''\n",
    "    for follicle_type, value in annotations.items():\n",
    "        if len(value[0]) != 0: #If there are coordinates for the follicle type\n",
    "            x_coords = []\n",
    "            y_coords = []\n",
    "            \n",
    "            #Separate coords into x and y values for plotting\n",
    "            for point in value[0]:\n",
    "                x_coords.append(point[0])\n",
    "                y_coords.append(point[1])\n",
    "                \n",
    "            #Plot the x and y coords as a scatter plot\n",
    "            ax.scatter(x_coords, y_coords, s = marker_size, c = value[1], marker = 'x', label = follicle_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f6ca4-7254-49eb-bbec-395c66791148",
   "metadata": {},
   "source": [
    "`plot_heatmap`: Plots the predictions as a Seaborn heatmap.\n",
    "\n",
    "<b>Parameters:</b>\\\n",
    "&emsp;`heatmap`: The predictions dataframe to be plotted as a heatmap.\n",
    "\n",
    "Uses the Seaborn heatmap function to plot the passed predictions dataframe, `heatmap`, as a heatmap.\\\n",
    "If `save_figs` is True, saves the heatmap to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53dc5af-27e0-481b-af99-a17c17a1b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(heatmap):\n",
    "    '''Creates and saves a heatmap from predictions DataFrame.'''\n",
    "    global SAVED_FILES\n",
    "\n",
    "    heatmap_shape = heatmap.shape\n",
    "    \n",
    "    #Plot heatmap\n",
    "    _, ax = plt.subplots(figsize = (10, 8))\n",
    "    \n",
    "    sns.heatmap(heatmap, cmap = COLOR_PALETTE, vmin = 0, vmax = 6)\n",
    "    \n",
    "    ax.set_xticks(range(0, heatmap_shape[1] + 1, 1000))\n",
    "    ax.set_yticks(range(0, heatmap_shape[0] + 1, 1000))\n",
    "    ax.set_xticklabels(range(0, heatmap_shape[1] + 1, 1000), rotation = 90)\n",
    "    ax.set_yticklabels(range(0, heatmap_shape[0] + 1, 1000))\n",
    "    ax.set_title(\"Heatmap of Predictions\")\n",
    "\n",
    "    #Memory management\n",
    "    del heatmap\n",
    "\n",
    "    #Save heatmap\n",
    "    if save_figs:\n",
    "        file = \"Heatmap.png\"\n",
    "\n",
    "        #Keep track of file\n",
    "        SAVED_FILES.append(file)\n",
    "\n",
    "        plt.savefig(file)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d603a-bd8b-4094-acb9-a7533e436edc",
   "metadata": {},
   "source": [
    "`plot_analysis`: Plots a four panel analysis of the orginal image and the predictions made on it.\n",
    "\n",
    "<b>Parameters:</b>\\\n",
    "&emsp;`image_file`: The file name of the image being predicted on.\\\n",
    "&emsp;`image_slice`: The slice of the image being predicted on.\\\n",
    "&emsp;`heatmap`: The prediction dataframe to be plotted as a heatmap.\\\n",
    "&emsp;`overlay`: The overlay image showing where on the original image predictions were made.\\\n",
    "&emsp;`pred_strength`: The dataframe containing maximum probabilities at each pixel, regardless of class.\\\n",
    "&emsp;`annotations`: Dictionary containing the coordinates of each human annotation for the image being predicted on, separated by follicle type.\n",
    "\n",
    "Loads in the original image using `read_image`. Then, plots the original image on the top-left panel.\\\n",
    "Uses the Seaborn heatmap function to plot the class predictions in `heatmap` as a heatmap on the top-right panel.\\\n",
    "Uses the Seaborn heatmap function to plot the maximum probabilities in `pred_strength` as a heatmap on the bottom-left panel.\\\n",
    "Plots the image overlay on the bottom-right panel. Additionally, uses `plot_annotations` to plot the human annotations in `annotations` on top of the overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19352cb0-2798-4662-b6cb-ef97b98962d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_analysis(image_file, image_slice, heatmap, overlay, pred_strength, annotations):\n",
    "    '''Create a plot consisting of the original image, prediction heatmap, heatmap overlay with annotations, and prediction strengths.\n",
    "    Save plot to a file.'''\n",
    "    global SAVED_FILES\n",
    "\n",
    "    img = read_image(image_file, image_slice = image_slice)\n",
    "    img_shape = img.shape\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize = (12, 12), gridspec_kw = {'width_ratios': [100, 5, 100, 5], 'height_ratios': [100, 100]})\n",
    "\n",
    "    #Plot original image\n",
    "    axes[0, 0].imshow(img)\n",
    "\n",
    "    axes[0, 0].set_xticks(range(0, img_shape[1] + 1, 1000))\n",
    "    axes[0, 0].set_yticks(range(0, img_shape[0] + 1, 1000))\n",
    "    axes[0, 0].set_xticklabels(range(0, img_shape[1] + 1, 1000), rotation = 90)\n",
    "    axes[0, 0].set_yticklabels(range(0, img_shape[0] + 1, 1000))\n",
    "    axes[0, 0].set_title(\"Original Image\")\n",
    "\n",
    "    #Memory management\n",
    "    del img\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    #Plot heatmap\n",
    "    sns.heatmap(heatmap, ax = axes[0, 2], cmap = COLOR_PALETTE, vmin = 0, vmax = 6, cbar_ax = axes[0, 3])\n",
    "    \n",
    "    axes[0, 2].set_xticks(range(0, img_shape[1] + 1, 1000))\n",
    "    axes[0, 2].set_yticks(range(0, img_shape[0] + 1, 1000))\n",
    "    axes[0, 2].set_xticklabels(range(0, img_shape[1] + 1, 1000), rotation = 90)\n",
    "    axes[0, 2].set_yticklabels(range(0, img_shape[0] + 1, 1000))\n",
    "    axes[0, 2].set_title(\"Heatmap\")\n",
    "\n",
    "    axes[0, 3].yaxis.set_ticks_position(\"left\")\n",
    "\n",
    "    #Memory management\n",
    "    del heatmap\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    #Plot prediction strengths\n",
    "    sns.heatmap(pred_strength, ax = axes[1, 0], vmin = 0.0, vmax = 1.0, cbar_ax = axes[1, 1])\n",
    "\n",
    "    axes[1, 0].set_xticks(range(0, img_shape[1] + 1, 1000))\n",
    "    axes[1, 0].set_yticks(range(0, img_shape[0] + 1, 1000))\n",
    "    axes[1, 0].set_xticklabels(range(0, img_shape[1] + 1, 1000), rotation = 90)\n",
    "    axes[1, 0].set_yticklabels(range(0, img_shape[0] + 1, 1000))\n",
    "    axes[1, 0].set_title(\"Prediction Strength\")\n",
    "\n",
    "    axes[1, 1].yaxis.set_ticks_position(\"left\")\n",
    "\n",
    "    #Memory management\n",
    "    del pred_strength\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    #Plot heatmap overlay and annotations\n",
    "    axes[1, 2].imshow(overlay)\n",
    "    plot_annotations(annotations, axes[1, 2], int((1 / overlay.shape[0]) * 100000))\n",
    "\n",
    "    axes[1, 2].set_xticks(range(0, img_shape[1] + 1, 1000))\n",
    "    axes[1, 2].set_yticks(range(0, img_shape[0] + 1, 1000))\n",
    "    axes[1, 2].set_xticklabels(range(0, img_shape[1] + 1, 1000), rotation = 90)\n",
    "    axes[1, 2].set_yticklabels(range(0, img_shape[0] + 1, 1000))\n",
    "    axes[1, 2].set_title(\"Heatmap Overlay with Annotations\")\n",
    "\n",
    "    #Memory management\n",
    "    del overlay\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    #Final touches\n",
    "    fig.delaxes(axes[0, 1])\n",
    "    fig.delaxes(axes[1, 3])\n",
    "    fig.suptitle(\"Prediction Analysis\")\n",
    "\n",
    "    #Save plot\n",
    "    if save_figs:\n",
    "        file = \"Analysis_Plot.png\"\n",
    "\n",
    "        #Keep track of file\n",
    "        SAVED_FILES.append(file)\n",
    "\n",
    "        plt.savefig(file)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b122ac63-5de1-4be5-adcd-971ec7943892",
   "metadata": {},
   "source": [
    "### Model Setup\n",
    "\n",
    "This section contains the setup for the image classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc976d6-9667-4bf2-bbe4-eea50ecdef6a",
   "metadata": {},
   "source": [
    "Set the device to use for PyTorch. Then print out the device to make sure it is using the correct one.\\\n",
    "&emsp;\"cuda\" = GPU, \"cpu\" = CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb556bbb-565a-4d32-8eb2-244d1a4bfb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a5a8ef-d172-4d89-adff-dea6113f92e8",
   "metadata": {},
   "source": [
    "Call `setup_model` and pass it the pretrained ResNet34 model to setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc6c1b-87bc-4548-8d73-d3b86b948f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model setup\n",
    "model = setup_model(models.resnet34(weights = \"ResNet34_Weights.DEFAULT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455cc5c1-1fdd-4ed1-a241-142c876453be",
   "metadata": {},
   "source": [
    "### Sample Image Setup\n",
    "\n",
    "This section sets up the sample image(s) to have predictions made on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b270ac8-fb35-48d2-bf15-7189a0014e35",
   "metadata": {},
   "source": [
    "Creates a dictionary of dictionaries that contains the images being predicted on and every slice being done on each image.\n",
    "\n",
    "<b>Dictionary Structure:</b>\n",
    "```python\n",
    "sample_imgs = {\n",
    "    \"image_file\": {\n",
    "        slice_number: {\n",
    "            'row': range object,\n",
    "            'col': range object\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6773a90b-86b5-4cfa-b429-bf3d0ff25403",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_imgs = {}\n",
    "\n",
    "#Loop through all sample images\n",
    "for image_file in SAMPLE_IMAGE_DICT.keys():\n",
    "    #If only doing some images, skip any images that are not in images_to_do\n",
    "    if do_certain_images and image_file not in images_to_do:\n",
    "        continue\n",
    "\n",
    "    #If doing the entire image, overwrite the row and collumn slices with a new range object that is the whole image with a 100 pixel buffer for subimages\n",
    "    if do_entire_image:\n",
    "        image = cv2.imread(SAMPLE_DIR + image_file)\n",
    "\n",
    "        row_slices[image_file] = [range(window_radius, image.shape[0] - window_radius)]\n",
    "        col_slices[image_file] = [range(window_radius, image.shape[1] - window_radius)]\n",
    "\n",
    "        #Memory management\n",
    "        del image\n",
    "\n",
    "    #Check if there is a row and collumn slice for an image if not doing entire image\n",
    "    if image_file not in row_slices.keys() or image_file not in col_slices.keys():\n",
    "        raise KeyError(\"Image \\'\" + image_file + \"\\' has not been given a slice and do_entire_image flag is set to False.\")\n",
    "\n",
    "    #Create a nested dictionary tied to the image file\n",
    "    sample_imgs[image_file] = {}\n",
    "    \n",
    "    #Loop through all row and col slices for the image\n",
    "    for i, (row_slice, col_slice) in enumerate(zip(row_slices[image_file], col_slices[image_file])):\n",
    "        if image_scaling_testing: #Rescale slices if doing image scaling\n",
    "            row_slice = range(int(row_slice[0] * multiplier), int((row_slice[-1] + 1) * multiplier))\n",
    "            col_slice = range(int(col_slice[0] * multiplier), int((col_slice[-1] + 1) * multiplier))\n",
    "\n",
    "        #Add row and collumn slices to another nested dictionary\n",
    "        sample_imgs[image_file][i] = {'row': row_slice, 'col': col_slice}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50dcfb9-0105-40b5-af48-37e0d59a7659",
   "metadata": {},
   "source": [
    "Plots all base images being predicted on. Uses `sample_imgs` dictionary made previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77887dfb-c1c6-42d6-b271-39d433d8d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine how many base images there are\n",
    "count = len(sample_imgs.keys())\n",
    "\n",
    "#Plot base images\n",
    "_, axes = plt.subplots(1, count, figsize = (count * 5, 5))\n",
    "\n",
    "if count < 2:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (image_file) in enumerate(sample_imgs):\n",
    "    image = read_image(image_file)\n",
    "\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(image_file)\n",
    "\n",
    "    #Memory management\n",
    "    del image\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4efce3-790a-4294-8e85-98f8a488d772",
   "metadata": {},
   "source": [
    "Plots all sliced images being predicted on. Uses `sample_imgs` dictionary made previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e976ee-0b74-4eba-88f5-53bb748469c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine how many image slices there are\n",
    "count = 0\n",
    "\n",
    "for values in sample_imgs.values():\n",
    "    for value in values.values():\n",
    "        count += 1\n",
    "\n",
    "#Plot the image slices\n",
    "_, axes = plt.subplots(1, count, figsize = (count * 5, 5))\n",
    "\n",
    "if count < 2:\n",
    "    axes = [axes]\n",
    "\n",
    "axis_num = 0\n",
    "\n",
    "for image_file, image_slices in sample_imgs.items():\n",
    "    image = read_image(image_file)\n",
    "\n",
    "    for slice_num, image_slice in image_slices.items():\n",
    "        row, col = image_slice['row'], image_slice['col']\n",
    "\n",
    "        image_sliced = image[row[0]:row[-1] + 1, col[0]:col[-1] + 1, :]\n",
    "\n",
    "        axes[axis_num].imshow(image_sliced)\n",
    "        axes[axis_num].set_title(image_file + \" Slice \" + str(slice_num + 1))\n",
    "\n",
    "        axis_num += 1\n",
    "\n",
    "    #Memory management\n",
    "    del image, image_sliced\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ffbe1-6c94-40d5-800b-48d5af6b8bba",
   "metadata": {},
   "source": [
    "#### Create Annotations Overlay for Sample Image Slice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba7c9b-e75a-4f8b-9c32-ae41c336b0bc",
   "metadata": {},
   "source": [
    "Finds and stores every coordinate for a human annotation on each image being predicted on that exists within each slice of the base image being done.\n",
    "\n",
    "<b>Dictionary Structure:</b>\n",
    "```python\n",
    "image_annot_coords = {\n",
    "    (slice_num, \"image_file\"): {\n",
    "        \"Primordial\": [[list of valid coordinates], COLOR_PALETTE[1]],\n",
    "        \"Transitional Primordial\": [[list of valid coordinates], COLOR_PALETTE[2]],\n",
    "        \"Primary\": [[list of valid coordinates], COLOR_PALETTE[3]],\n",
    "        \"Transitional Primary\": [[list of valid coordinates], COLOR_PALETTE[4]],\n",
    "        \"Secondary\": [[list of valid coordinates], COLOR_PALETTE[5]],\n",
    "        \"Multilayer\": [[list of valid coordinates], COLOR_PALETTE[6]]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb51d6-2bc1-41bc-a0b6-dae0bc54a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_annot_coords = {}\n",
    "\n",
    "#Loop through every image and slice being done\n",
    "for image_file, image_slices in sample_imgs.items():\n",
    "    #Load in base image's annotations\n",
    "    annotations = pd.read_csv(SAMPLE_DIR + SAMPLE_IMAGE_DICT[image_file], sep = '\\t')\n",
    "\n",
    "    #Convert microns to pixels\n",
    "    annotations[[\"Centroid X px\", \"Centroid Y px\"]] = annotations[['Centroid X m', 'Centroid Y m']] / 0.69\n",
    "\n",
    "    if image_file == \"DP28_25081_Section3_10X_ome_copy.tif\": #One of the sample images has a different scale\n",
    "        annotations[[\"Centroid X px\", \"Centroid Y px\"]] = annotations[['Centroid X m', 'Centroid Y m']] / 0.1725\n",
    "\n",
    "    if image_scaling_testing: #If scaling images\n",
    "        annotations[['Centroid X px', 'Centroid Y px']] = annotations[['Centroid X px', 'Centroid Y px']] * multiplier\n",
    "    \n",
    "    #Remove any collumn in the dataframe that is not needed and remove na values\n",
    "    annotations = annotations[['Name', 'Centroid X px', 'Centroid Y px']]\n",
    "    annotations.dropna(axis = 0, inplace = True, ignore_index = True)\n",
    "\n",
    "    #Loop through each image slice\n",
    "    for slice_num, image_slice in image_slices.items():\n",
    "        row_slice, col_slice = image_slice['row'], image_slice['col']\n",
    "    \n",
    "        annot_coords = {\n",
    "            \"Primordial\": [[], COLOR_PALETTE[1]],\n",
    "            \"Transitional Primordial\": [[], COLOR_PALETTE[2]],\n",
    "            \"Primary\": [[], COLOR_PALETTE[3]],\n",
    "            \"Transitional Primary\": [[], COLOR_PALETTE[4]],\n",
    "            \"Secondary\": [[], COLOR_PALETTE[5]],\n",
    "            \"Multilayer\": [[], COLOR_PALETTE[6]]\n",
    "        }\n",
    "    \n",
    "        #Loop through each annotation coordinate and check if they are within the bounds of the image slice\n",
    "        for i in range(len(annotations)):\n",
    "            annot_class = annotations['Name'][i]\n",
    "                \n",
    "            match annot_class:\n",
    "                case 'Primordial':\n",
    "                    if check_coord_bounds(annotations, row_slice, col_slice, i):\n",
    "                        annot_coords[annot_class][0].append(get_coords(annotations, row_slice, col_slice, i))\n",
    "                case 'Transitional Primordial':\n",
    "                    if check_coord_bounds(annotations, row_slice, col_slice, i):\n",
    "                        annot_coords[annot_class][0].append(get_coords(annotations, row_slice, col_slice, i))\n",
    "                case 'Primary':\n",
    "                    if check_coord_bounds(annotations, row_slice, col_slice, i):\n",
    "                        annot_coords[annot_class][0].append(get_coords(annotations, row_slice, col_slice, i))\n",
    "                case 'Transitional Primary':\n",
    "                    if check_coord_bounds(annotations, row_slice, col_slice, i):\n",
    "                        annot_coords[annot_class][0].append(get_coords(annotations, row_slice, col_slice, i))\n",
    "                case 'Secondary':\n",
    "                    if check_coord_bounds(annotations, row_slice, col_slice, i):\n",
    "                        annot_coords[annot_class][0].append(get_coords(annotations, row_slice, col_slice, i))\n",
    "                case 'Multilayer':\n",
    "                    if check_coord_bounds(annotations, row_slice, col_slice, i):\n",
    "                        annot_coords[annot_class][0].append(get_coords(annotations, row_slice, col_slice, i))\n",
    "\n",
    "        image_annot_coords[(slice_num, image_file)] = annot_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c1831-0f43-48a1-a686-3c865f367b6a",
   "metadata": {},
   "source": [
    "Plots the annotation coordinates as a scatter plot over each image slice, respectively. Uses both `sample_imgs` and `image_annot_coords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f097bc-a734-48fc-b2a2-fec84c3d8165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine how many images slices are being done\n",
    "count = 0\n",
    "\n",
    "for values in sample_imgs.values():\n",
    "    for value in values.values():\n",
    "        count += 1\n",
    "\n",
    "#Plot the image slices\n",
    "_, axes = plt.subplots(1, count, figsize = (count * 5, 5))\n",
    "\n",
    "if count < 2:\n",
    "    axes = [axes]\n",
    "\n",
    "axis_num = 0\n",
    "\n",
    "for image_file, image_slices in sample_imgs.items():\n",
    "    image = read_image(image_file)\n",
    "\n",
    "    for slice_num, image_slice in image_slices.items():\n",
    "        row, col = image_slice['row'], image_slice['col']\n",
    "\n",
    "        image_sliced = image[row[0]:row[-1] + 1, col[0]:col[-1] + 1, :]\n",
    "\n",
    "        axes[axis_num].imshow(image_sliced)\n",
    "        axes[axis_num].set_title(image_file + \" Slice \" + str(slice_num + 1))\n",
    "        #Additionally, scatter plot the annotation coordinates over each image slice\n",
    "        plot_annotations(image_annot_coords[(slice_num, image_file)], axes[axis_num], int((1 / image_sliced.shape[0]) * 100000))\n",
    "\n",
    "        axis_num += 1\n",
    "\n",
    "    #Memory management\n",
    "    del image, image_sliced\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b920a47-057a-4fe4-96ac-3671b40c4ae6",
   "metadata": {},
   "source": [
    "## Main Code\n",
    "\n",
    "This section contains the main code of the program that makes predictions on the images and creates analysis plots to examine those predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b78ddb-74d2-4907-b85c-3a6cc6986a9e",
   "metadata": {},
   "source": [
    "If the `multiple_epochs` flag is set to False, load the checkpoint that is being used to make predictions. Then, loop through every image/slice that is having predictions made. Make predictions on the image/slice. Then, make an overlay out of the predictions and make a dataframe of maximum probabilities out of the class probabilities for each prediction. Plot the predictions as a heatmap on its own, then plot the full, four panel analysis plot using the original image/slice, the predictions, the maximum probabilities, and the human annotations. Finally, make an output directory for this image/slice and move any related files that were created into the output directory. Once the image/slice is finished, move on to the next one. If all images/slices have been finished, \"Done\" will be printed to the cell output.\n",
    "\n",
    "If the `multiple_epochs` flag is set to True, loop through every epoch, loading the checkpoint for that epoch at the beginning of the iteration. Then, loop through every image/slice that is having predictions made. Make predictions on the image/slice. Then, make an overlay out of the predictions and make a dataframe of maximum probabilities out of the class probabilities for each prediction. Plot the predictions as a heatmap on its own, then plot the full, four panel analysis plot using the original image/slice, the predictions, the maximum probabilities, and the human annotations. Finally, make an output directory for this image/slice and move any related files that were created into the output directory. Once the image/slice is finished, move on to the next one. If all images/slices have been finished, move on to the next epoch. Once all epochs are finished, \"Done\" will be printed to the cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60e9a1-1bf4-4a7f-afee-74942997456d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not multiple_epochs: #Only one epoch\n",
    "    #Load checkpoint\n",
    "    model.load_state_dict(torch.load(CHECKPOINT + str(checkpoint_name) + \".pt\", map_location = device, weights_only = True))\n",
    "    \n",
    "    #Loop through all images and slices being done\n",
    "    for image_file, image_slices in sample_imgs.items():\n",
    "        for slice_num, image_slice in image_slices.items():\n",
    "            #Memory management\n",
    "            gc.collect()\n",
    "            \n",
    "            #Trach start time\n",
    "            start_time = datetime.datetime.now()\n",
    "            print(\"---------- Slice: {} of Image: {} ----------\".format(slice_num + 1, image_file))\n",
    "    \n",
    "            #Make predictions on the image slice\n",
    "            probabilities, predictions, pred_time = make_predictions(image_slice['row'], image_slice['col'], image_file)\n",
    "            \n",
    "            #Create an overlay showing where the model made predictions\n",
    "            overlay_img = create_overlay(predictions, image_file, image_slice)\n",
    "            \n",
    "            #Create a dataframe that shows how strong the model's predictions were at each pixel, regardless of predicted class\n",
    "            prediction_strengths = create_probability_data(probabilities)\n",
    "    \n",
    "            #Memory management\n",
    "            del probabilities\n",
    "            \n",
    "            #Plot heatmap and full analysis plots\n",
    "            plot_heatmap(predictions)\n",
    "            plot_analysis(image_file, image_slice, predictions, overlay_img, prediction_strengths, image_annot_coords[(slice_num, image_file)])\n",
    "    \n",
    "            #Memory management\n",
    "            del predictions, overlay_img, prediction_strengths\n",
    "        \n",
    "            print(\"  ---------- Saving and Moving Files ----------\")\n",
    "                \n",
    "            #Create output directory and move files\n",
    "            make_output_dir(slice_num, image_file)\n",
    "            print(\"    Output Dir: {}\".format(TIME_STAMP_OUTPUT_DIR))\n",
    "        \n",
    "            shutil.copy2(CHECKPOINT + str(checkpoint_name) + \".pt\", TIME_STAMP_OUTPUT_DIR)\n",
    "        \n",
    "            for file in SAVED_FILES:\n",
    "                shutil.move(file, TIME_STAMP_OUTPUT_DIR)\n",
    "        \n",
    "            SAVED_FILES.clear()\n",
    "        \n",
    "            APP.commands.execute(\"docmanager:save\")\n",
    "        \n",
    "            shutil.copy2(NOTEBOOK_NAME, TIME_STAMP_OUTPUT_DIR)\n",
    "        \n",
    "            #Prevents html errors in notebook\n",
    "            clear_output(wait = True)\n",
    "        \n",
    "            #Track end time\n",
    "            end_time = datetime.datetime.now()\n",
    "            #Save prediction time and total time taken to a dictionary\n",
    "            RUNTIMES[(slice_num, image_file)] = {\"prediction\": str(pred_time - start_time).split(\".\")[0], \"total\": str(end_time - start_time).split(\".\")[0]}\n",
    "    \n",
    "    print(\"Done\")\n",
    "elif multiple_epochs: #Doing each saved epoch from program 3 when save_each_epoch is True\n",
    "    #Loop through each epoch\n",
    "    for epoch in range(0, num_epochs + 1):\n",
    "        #Load checkpoint\n",
    "        model.load_state_dict(torch.load(CHECKPOINT + str(epoch) + \".pt\", map_location = device, weights_only = True))\n",
    "        \n",
    "        #Loop through all images and slices being done\n",
    "        for image_file, image_slices in sample_imgs.items():\n",
    "            for slice_num, image_slice in image_slices.items():\n",
    "                #Memory management\n",
    "                gc.collect()\n",
    "\n",
    "                print(\"---------- Epoch: {} of {} ----------\".format(epoch, num_epochs))\n",
    "                \n",
    "                #Trach start time\n",
    "                start_time = datetime.datetime.now()\n",
    "                print(\"  ---------- Slice: {} of Image: {} ----------\".format(slice_num + 1, image_file))\n",
    "        \n",
    "                #Make predictions on the image slice\n",
    "                probabilities, predictions, pred_time = make_predictions(image_slice['row'], image_slice['col'], image_file)\n",
    "                \n",
    "                #Create an overlay showing where the model made predictions\n",
    "                overlay_img = create_overlay(predictions, image_file, image_slice)\n",
    "                \n",
    "                #Create a dataframe that shows how strong the model's predictions were at each pixel, regardless of predicted class\n",
    "                prediction_strengths = create_probability_data(probabilities)\n",
    "        \n",
    "                #Memory management\n",
    "                del probabilities\n",
    "                \n",
    "                #Plot heatmap and full analysis plots\n",
    "                plot_heatmap(predictions)\n",
    "                plot_analysis(image_file, image_slice, predictions, overlay_img, prediction_strengths, image_annot_coords[(slice_num, image_file)])\n",
    "        \n",
    "                #Memory management\n",
    "                del predictions, overlay_img, prediction_strengths\n",
    "            \n",
    "                print(\"    ---------- Saving and Moving Files ----------\")\n",
    "                    \n",
    "                #Create output directory and move files\n",
    "                make_output_dir(slice_num, image_file, epoch)\n",
    "                print(\"      Output Dir: {}\".format(TIME_STAMP_OUTPUT_DIR))\n",
    "            \n",
    "                shutil.copy2(CHECKPOINT + str(epoch) + \".pt\", TIME_STAMP_OUTPUT_DIR)\n",
    "            \n",
    "                for file in SAVED_FILES:\n",
    "                    shutil.move(file, TIME_STAMP_OUTPUT_DIR)\n",
    "            \n",
    "                SAVED_FILES.clear()\n",
    "            \n",
    "                APP.commands.execute(\"docmanager:save\")\n",
    "            \n",
    "                shutil.copy2(NOTEBOOK_NAME, TIME_STAMP_OUTPUT_DIR)\n",
    "            \n",
    "                #Prevents html errors in notebook\n",
    "                clear_output(wait = True)\n",
    "            \n",
    "                #Track end time\n",
    "                end_time = datetime.datetime.now()\n",
    "                #Save prediction time and total time taken to a dictionary\n",
    "                RUNTIMES[(\"Epoch \" + str(epoch), slice_num, image_file)] = {\"prediction\": str(pred_time - start_time).split(\".\")[0], \"total\": str(end_time - start_time).split(\".\")[0]}\n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045cb764-b6e2-415c-bca5-0b6af8c2c802",
   "metadata": {},
   "source": [
    "Prints out the `RUNTIMES` dictionary, showing how long each image/slice took. Each image/slice has a prediction time, which is how long it took to only make predictions on the image, and a total time, which is how long the entire iteration of the loop for that image/slice took including prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b34954-d27e-407b-ab48-73b335c9cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the runtimes for each image/slice\n",
    "for key, value in RUNTIMES.items():\n",
    "    print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab90d4-41d0-4a0c-b575-6d303479d816",
   "metadata": {},
   "source": [
    "### Convert Notebook to HTML and Move to Output Directory / Clean Up Working Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460074d-8bdb-46e6-b752-d73990f28a32",
   "metadata": {},
   "source": [
    "Programmatically save the notebook, convert it to html and move the html file to the base output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61c2bd-908d-4082-b556-4a908ce88589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For some reason, this is needed for the save command just below it to function\n",
    "time.sleep(1)\n",
    "\n",
    "#Save notebook\n",
    "APP.commands.execute(\"docmanager:save\")\n",
    "\n",
    "#Convert notebook to html\n",
    "!jupyter nbconvert --to html \"$NOTEBOOK_NAME\"\n",
    "\n",
    "#Move html and copy notebook to output directory\n",
    "shutil.move(NOTEBOOK_NAME[:-6] + \".html\", OUTPUT_DIR + NOTEBOOK_NAME[:-6] + \"_\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704f00c-6ff9-4e78-9a98-0c268459eda5",
   "metadata": {},
   "source": [
    "Final cleanup of current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e4eda4-11e3-4619-bbea-b2b1ca9ef8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove remaining checkpoint(s)\n",
    "if not multiple_epochs:\n",
    "    os.remove(CHECKPOINT + str(checkpoint_name) + \".pt\")\n",
    "elif multiple_epochs:\n",
    "    for epoch in range(0, num_epochs + 1):\n",
    "        os.remove(CHECKPOINT + str(epoch) + \".pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
